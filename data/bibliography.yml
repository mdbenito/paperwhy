---
references:
- id: lin_why_2016
  type: article-journal
  author:
  - family: Lin
    given: Henry W.
  - family: Tegmark
    given: Max
  issued:
  - year: '2016'
    month: '8'
  accessed:
  - year: '2016'
    month: '10'
    day: '26'
  title: Why does deep and cheap learning work so well?
  container-title: arXiv:1608.08225 \[cond-mat, stat\]
  page: '17'
  note: 'citecount: 00019 arXiv: 1608.08225'
  abstract: 'We show how the success of deep learning depends not only on mathematics
    but also on physics: although well-known mathematical theorems guarantee that
    neural networks can approximate arbitrary functions well, the class of functions
    of practical interest can be approximated through “cheap learning” with exponentially
    fewer parameters than generic ones, because they have simplifying properties tracing
    back to the laws of physics. The exceptional simplicity of physics-based functions
    hinges on properties such as symmetry, locality, compositionality and polynomial
    log-probability, and we explore how these properties translate into exceptionally
    simple neural networks approximating both natural phenomena such as images and
    abstract representations thereof such as drawings. We further argue that when
    the statistical process generating the data is of a certain hierarchical form
    prevalent in physics and machine-learning, a deep neural network can be more efficient
    than a shallow one. We formalize these claims using information theory and discuss
    the relation to renormalization group procedures. We prove various “no-flattening
    theorems” showing when such efficient deep networks cannot be accurately approximated
    by shallow ones without efficiency loss: flattening even linear functions can
    be costly, and flattening polynomials is exponentially expensive; we use group
    theoretic techniques to show that n variables cannot be multiplied using fewer
    than 2\^n neurons in a single hidden layer.'
  URL: http://arxiv.org/abs/1608.08225

- id: bartlett_representational_2017
  type: no-type
  author:
  - family: Bartlett
    given: Peter
  issued:
  - year: '2017'
    month: '5'
  accessed:
  - year: '2017'
    month: '5'
    day: '2'
  title: Representational and optimization properties of Deep Residual Networks
  publisher-place: Simons Institute
  genre: Talk
  note: 'citecount: 00000 2681 seconds'
  URL: https://simons.berkeley.edu/talks/tba-1

- id: hinton_improving_2012
  type: article-journal
  author:
  - family: Hinton
    given: Geoffrey E.
  - family: Srivastava
    given: Nitish
  - family: Krizhevsky
    given: Alex
  - family: Sutskever
    given: Ilya
  - family: Salakhutdinov
    given: Ruslan R.
  issued:
  - year: '2012'
    month: '7'
  accessed:
  - year: '2017'
    month: '4'
    day: '28'
  title: Improving neural networks by preventing co-adaptation of feature detectors
  container-title: arXiv:1207.0580 \[cs\]
  page: '18'
  note: 'citecount: 01870 arXiv: 1207.0580'
  abstract: When a large feedforward neural network is trained on a small training
    set, it typically performs poorly on held-out test data. This “overfitting” is
    greatly reduced by randomly omitting half of the feature detectors on each training
    case. This prevents complex co-adaptations in which a feature detector is only
    helpful in the context of several other specific feature detectors. Instead, each
    neuron learns to detect a feature that is generally helpful for producing the
    correct answer given the combinatorially large variety of internal contexts in
    which it must operate. Random “dropout” gives big improvements on many benchmark
    tasks and sets new records for speech and object recognition.
  URL: http://arxiv.org/abs/1207.0580

- id: baldi_dropout_2014
  type: article-journal
  author:
  - family: Baldi
    given: Pierre
  - family: Sadowski
    given: Peter
  issued:
  - year: '2014'
    month: '5'
  accessed:
  - year: '2017'
    month: '5'
    day: '2'
  title: The Dropout Learning Algorithm
  container-title: Artificial intelligence
  page: '78-122'
  volume: '210'
  note: 'citecount: 00068'
  abstract: 'Dropout is a recently introduced algorithm for training neural network
    by randomly dropping units during training to prevent their co-adaptation. A mathematical
    analysis of some of the static and dynamic properties of dropout is provided using
    Bernoulli gating variables, general enough to accommodate dropout on units or
    connections, and with variable rates. The framework allows a complete analysis
    of the ensemble averaging properties of dropout in linear networks, which is useful
    to understand the non-linear case. The ensemble averaging properties of dropout
    in non-linear logistic networks result from three fundamental equations: (1) the
    approximation of the expectations of logistic functions by normalized geometric
    means, for which bounds and estimates are derived; (2) the algebraic equality
    between normalized geometric means of logistic functions with the logistic of
    the means, which mathematically characterizes logistic functions; and (3) the
    linearity of the means with respect to sums, as well as products of independent
    variables. The results are also extended to other classes of transfer functions,
    including rectified linear functions. Approximation errors tend to cancel each
    other and do not accumulate. Dropout can also be connected to stochastic neurons
    and used to predict firing rates, and to backpropagation by viewing the backward
    propagation as ensemble averaging in a dropout linear network. Moreover, the convergence
    properties of dropout can be understood in terms of stochastic gradient descent.
    Finally, for the regularization properties of dropout, the expectation of the
    dropout gradient is the gradient of the corresponding approximation ensemble,
    regularized by an adaptive weight decay term with a propensity for self-consistent
    variance minimization and sparse representations.'
  URL: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3996711/
  DOI: 10.1016/j.artint.2014.02.004
  ISSN: '0004-3702'
  PMCID: PMC3996711
  PMID: '24771879'

- id: tang_deep_2013
  type: article-journal
  author:
  - family: Tang
    given: Yichuan
  issued:
  - year: '2013'
    month: '6'
  accessed:
  - year: '2015'
    month: '4'
    day: '29'
  title: Deep learning using linear Support Vector Machines
  container-title: arXiv:1306.0239 \[cs, stat\]
  page: '6'
  note: 'citecount: 00164 arXiv: 1306.0239'
  abstract: Recently, fully-connected and convolutional neural networks have been
    trained to achieve state-of-the-art performance on a wide variety of tasks such
    as speech recognition, image classification, natural language processing, and
    bioinformatics. For classification tasks, most of these “deep learning” models
    employ the softmax activation function for prediction and minimize cross-entropy
    loss. In this paper, we demonstrate a small but consistent advantage of replacing
    the softmax layer with a linear support vector machine. Learning minimizes a margin-based
    loss instead of the cross-entropy loss. While there have been various combinations
    of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply
    replacing softmax with linear SVMs gives significant gains on popular deep learning
    datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop’s
    face expression recognition challenge.
  URL: http://arxiv.org/abs/1306.0239

- id: goodfellow_maxout_2013
  type: article-journal
  author:
  - family: Goodfellow
    given: Ian J.
  - family: Warde-Farley
    given: David
  - family: Mirza
    given: Mehdi
  - family: Courville
    given: Aaron
  - family: Bengio
    given: Yoshua
  issued:
  - year: '2013'
    month: '2'
  accessed:
  - year: '2017'
    month: '4'
    day: '18'
  title: Maxout Networks
  container-title: arXiv:1302.4389 \[cs, stat\]
  page: '9'
  note: 'citecount: 00756 code: http://www-etud.iro.umontreal.ca/\~goodfeli/maxout.html
    arxiv: 1302.4389'
  abstract: 'We consider the problem of designing models to leverage a recently
    introduced approximate model averaging technique called dropout. We define a simple
    new model called maxout (so named because its output is the max of a set of inputs,
    and because it is a natural companion to dropout) designed to both facilitate
    optimization by dropout and improve the accuracy of dropout’s fast approximate
    model averaging technique. We empirically verify that the model successfully accomplishes
    both of these tasks. We use maxout and dropout to demonstrate state of the art
    classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100,
    and SVHN.'
  URL: http://arxiv.org/abs/1302.4389

- id: ng_spectral_2002
  type: chapter
  author:
  - family: Ng
    given: Andrew Y.
  - family: Jordan
    given: Michael I.
  - family: Weiss
    given: Yair
  editor:
  - family: Dietterich
    given: T. G.
  - family: Becker
    given: S.
  - family: Ghahramani
    given: Z.
  issued:
  - year: '2002'
  accessed:
  - year: '2017'
    month: '4'
    day: '12'
  title: 'On Spectral Clustering: Analysis and an algorithm'
  title-short: On Spectral Clustering
  container-title: Advances in Neural Information Processing Systems 14
  publisher: MIT Press
  page: '849-856'
  note: 'citecount: 05818'
  URL: http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf

- id: nevo_identifying_2017
  type: article-journal
  author:
  - family: Nevo
    given: Daniel
  - family: Ritov
    given: Ya’acov
  issued:
  - year: '2017'
  accessed:
  - year: '2017'
    month: '4'
    day: '16'
  title: Identifying a minimal class of models for high-dimensional data
  container-title: Journal of Machine Learning Research
  page: '1-29'
  volume: '18'
  issue: '24'
  note: 'citecount: 00000 arXiv:1504.00494'
  URL: https://paperwhy.aerobatic.io/2017/05/18/deep-sparse-rectifier-neural-networks/

- id: swirszcz_local_2016
  type: article-journal
  author:
  - family: Swirszcz
    given: Grzegorz
  - family: Czarnecki
    given: Wojciech Marian
  - family: Pascanu
    given: Razvan
  issued:
  - year: '2016'
    month: '11'
  accessed:
  - year: '2017'
    month: '5'
    day: '3'
  title: Local minima in training of neural networks
  container-title: arXiv:1611.06310 \[cs, stat\]
  page: '12'
  note: 'citecount: 00001 arXiv: 1611.06310'
  abstract: There has been a lot of recent interest in trying to characterize the
    error surface of deep models. This stems from a long standing question. Given
    that deep networks are highly nonlinear systems optimized by local gradient methods,
    why do they not seem to be affected by bad local minima? It is widely believed
    that training of deep models using gradient methods works so well because the
    error surface either has no local minima, or if they exist they need to be close
    in value to the global minimum. It is known that such results hold under very
    strong assumptions which are not satisfied by real models. In this paper we present
    examples showing that for such theorem to be true additional assumptions on the
    data, initialization schemes and/or the model classes have to be made. We look
    at the particular case of finite size datasets. We demonstrate that in this scenario
    one can construct counter-examples (datasets or initialization schemes) when the
    network does become susceptible to bad local minima over the weight space.
  URL: http://arxiv.org/abs/1611.06310

- id: athiwaratkun_multimodal_2017
  type: article-journal
  author:
  - family: Athiwaratkun
    given: Ben
  - family: Wilson
    given: Andrew Gordon
  issued:
  - year: '2017'
    month: '4'
  accessed:
  - year: '2017'
    month: '5'
    day: '25'
  title: Multimodal Word Distributions
  container-title: arXiv:1704.08424 \[cs, stat\]
  note: 'citecount: 00000 arXiv: 1704.08424'
  abstract: Word embeddings provide point representations of words containing useful
    semantic information. We introduce multimodal word distributions formed from Gaussian
    mixtures, for multiple word meanings, entailment, and rich uncertainty information.
    To learn these distributions, we propose an energy-based max-margin objective.
    We show that the resulting approach captures uniquely expressive semantic information,
    and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings,
    on benchmark datasets such as word similarity and entailment.
  URL: http://arxiv.org/abs/1704.08424

- id: bengio_greedy_2007
  type: chapter
  author:
  - family: Bengio
    given: Yoshua
  - family: Lamblin
    given: Pascal
  - family: Popovici
    given: Dan
  - family: Larochelle
    given: Hugo
  editor:
  - family: Schölkopf
    given: P. B.
  - family: Platt
    given: J. C.
  - family: Hoffman
    given: T.
  issued:
  - year: '2007'
  accessed:
  - year: '2017'
    month: '5'
    day: '13'
  title: Greedy layer-wise training of Deep Networks
  container-title: Advances in Neural Information Processing Systems 19
  publisher: MIT Press
  page: '153-160'
  note: 'citecount: 02175'
  URL: http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf

- id: breiman_bagging_1996
  type: article-journal
  author:
  - family: Breiman
    given: Leo
  issued:
  - year: '1996'
    month: '8'
  accessed:
  - year: '2017'
    month: '5'
    day: '23'
  title: Bagging predictors
  container-title: Machine Learning
  page: '123-140'
  volume: '24'
  issue: '2'
  note: 'citecount: 16604'
  abstract: Bagging predictors is a method for generating multiple versions of a predictor
    and using these to get an aggregated predictor. The aggregation averages over
    the versions when predicting a numerical outcome and does a plurality vote when
    predicting a class. The multiple versions are formed by making bootstrap replicates
    of the learning set and using these as new learning sets. Tests on real and simulated
    data sets using classification and regression trees and subset selection in linear
    regression show that bagging can give substantial gains in accuracy. The vital
    element is the instability of the prediction method. If perturbing the learning
    set can cause significant changes in the predictor constructed, then bagging can
    improve accuracy.
  URL: https://link.springer.com/article/10.1023/A:1018054314350
  DOI: 10.1023/A:1018054314350
  ISSN: 0885-6125, 1573-0565

- id: vilnis_word_2014
  type: article-journal
  author:
  - family: Vilnis
    given: Luke
  - family: McCallum
    given: Andrew
  issued:
  - year: '2014'
    month: '12'
  accessed:
  - year: '2017'
    month: '5'
    day: '25'
  title: Word Representations via Gaussian Embedding
  container-title: arXiv:1412.6623 \[cs\]
  note: 'citecount: 00043 arXiv: 1412.6623'
  abstract: Current work in lexical distributed representations maps each word to
    a point vector in low-dimensional space. Mapping instead to a density provides
    many interesting advantages, including better capturing uncertainty about a representation
    and its relationships, expressing asymmetries more naturally than dot product
    or cosine similarity, and enabling more expressive parameterization of decision
    boundaries. This paper advocates for density-based distributed embeddings and
    presents a method for learning representations in the space of Gaussian distributions.
    We compare performance on various word embedding benchmarks, investigate the ability
    of these embeddings to model entailment and other asymmetric relationships, and
    explore novel properties of the representation.
  URL: http://arxiv.org/abs/1412.6623

- id: glorot_deep_2011
  type: paper-conference
  author:
  - family: Glorot
    given: Xavier
  - family: Bordes
    given: Antoine
  - family: Bengio
    given: Yoshua
  issued:
  - year: '2011'
    month: '4'
  accessed:
  - year: '2017'
    month: '4'
    day: '7'
  title: Deep Sparse Rectifier Neural Networks
  container-title: Proceedings of the Fourteenth International Conference on Artificial
    Intelligence and Statistics April 11-13, 2011, Fort Lauderdale, FL, USA
  publisher-place: Florida, USA
  page: '315-323'
  volume: '15'
  note: 'citecount: 00935'
  abstract: While logistic sigmoid neurons are more biologically plausible than hyperbolic
    tangent neurons, the latter work better for training multi-layer neural networks.
    This paper shows that rectifying neurons are an even better model of biological
    neurons and yield equal or better performance than hyperbolic tangent networks
    in spite of the hard non-linearity and non-differentiability at zero, creating
    sparse representations with true zeros which seem remarkably suitable for naturally
    sparse data. Even though they can take advantage of semi-supervised setups with
    extra-unlabeled data, deep rectifier networks can reach their best performance
    without requiring any unsupervised pre-training on purely supervised tasks with
    large labeled datasets. Hence, these results can be seen as a new milestone in
    the attempts at understanding the difficulty in training deep but purely supervised
    neural networks, and closing the performance gap between neural networks learnt
    with and without unsupervised pre-training.
  URL: http://www.jmlr.org/proceedings/papers/v15/

- id: baldi_understanding_2013
  type: chapter
  author:
  - family: Baldi
    given: Pierre
  - family: Sadowski
    given: Peter J
  editor:
  - family: Burges
    given: C. J. C.
  - family: Bottou
    given: L.
  - family: Welling
    given: M.
  - family: Ghahramani
    given: Z.
  - family: Weinberger
    given: K. Q.
  issued:
  - year: '2013'
  title: Understanding Dropout
  container-title: Advances in Neural Information Processing Systems 26
  page: '2814-2822'
  note: 'citecount: 00066'
  abstract: Dropout is a relatively new algorithm for training neural networks which
    relies on stochastically dropping out neurons during training in order to avoid
    the co-adaptation of feature detectors. We introduce a general formalism for studying
    dropout on either units or connections, with arbitrary probability values, and
    use it to analyze the averaging and regularizing properties of dropout in both
    linear and non-linear networks. For deep neural networks, the averaging properties
    of dropout are characterized by three recursive equations, including the approximation
    of expectations by normalized weighted geometric means. We provide estimates and
    bounds for these approximations and corroborate the results with simulations.
    We also show in simple cases how dropout performs stochastic gradient descent
    on a regularized error function.
  URL: http://papers.nips.cc/paper/4878-understanding-dropout.pdf

- id: nair_rectified_2010
  type: paper-conference
  author:
  - family: Nair
    given: Vinod
  - family: Hinton
    given: Geoffrey E
  issued:
  - year: '2010'
  title: Rectified linear units improve restricted boltzmann machines
  container-title: Proceedings of the 27th international conference on machine learning
    (ICML-10)
  publisher-place: Haifa, Israel
  page: '807-814'
  note: 'citecount: 01616'

- id: arias-castro_spectral_2017
  type: article-journal
  author:
  - family: Arias-Castro
    given: Ery
  - family: Lerman
    given: Gilad
  - family: Zhang
    given: Teng
  issued:
  - year: '2017'
  accessed:
  - year: '2017'
    month: '4'
    day: '12'
  title: Spectral Clustering based on local PCA
  container-title: Journal of Machine Learning Research
  page: '1-57'
  volume: '18'
  issue: '9'
  note: 'citecount: 00025 code: http://sciences.ucf.edu/math/tengz'
  URL: http://jmlr.org/papers/v18/14-318.html

- id: he_deep_2016
  type: paper-conference
  author:
  - family: He
    given: Kaiming
  - family: Zhang
    given: Xiangyu
  - family: Ren
    given: Shaoqing
  - family: Sun
    given: Jian
  issued:
  - year: '2016'
  accessed:
  - year: '2017'
    month: '5'
    day: '2'
  title: Deep Residual Learning for Image Recognition
  page: '770-778'
  note: 'citecount: 02156 Extended version with appendix from the Arxiv'
  abstract: Deeper neural networks are more difficult to train. We present a residual
    learning framework to ease the training of networks that are substantially deeper
    than those used previously. We explicitly reformulate the layers as learning residual
    functions with reference to the layer inputs, instead of learning unreferenced
    functions. We provide comprehensive empirical evidence showing that these residual
    networks are easier to optimize, and can gain accuracy from considerably increased
    depth. On the ImageNet dataset we evaluate residual nets with a depth of up to
    152 layers—8x deeper than VGG nets but still having lower complexity. An ensemble
    of these residual nets achieves 3.57% error on the ImageNet test set. This result
    won the 1st place on the ILSVRC 2015 classification task. We also present analysis
    on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central
    importance for many visual recognition tasks. Solely due to our extremely deep
    representations, we obtain a 28% relative improvement on the COCO object detection
    dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO
    2015 competitions, where we also won the 1st places on the tasks of ImageNet detection,
    ImageNet localization, COCO detection, and COCO segmentation.
  URL: http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html

- id: he_identity_2016
  type: article-journal
  author:
  - family: He
    given: Kaiming
  - family: Zhang
    given: Xiangyu
  - family: Ren
    given: Shaoqing
  - family: Sun
    given: Jian
  issued:
  - year: '2016'
    month: '3'
  accessed:
  - year: '2017'
    month: '5'
    day: '2'
  title: Identity Mappings in Deep Residual Networks
  container-title: arXiv:1603.05027 \[cs\]
  page: '15'
  note: 'citecount: 00247 arXiv: 1603.05027'
  abstract: 'Deep residual networks have emerged as a family of extremely deep architectures
    showing compelling accuracy and nice convergence behaviors. In this paper, we
    analyze the propagation formulations behind the residual building blocks, which
    suggest that the forward and backward signals can be directly propagated from
    one block to any other block, when using identity mappings as the skip connections
    and after-addition activation. A series of ablation experiments support the importance
    of these identity mappings. This motivates us to propose a new residual unit,
    which makes training easier and improves generalization. We report improved results
    using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer
    ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers'
  URL: http://arxiv.org/abs/1603.05027

- id: schmidt_minimizing_2016
  type: manuscript
  author:
  - family: Schmidt
    given: Mark
  - family: Le Roux
    given: Nicolas
  - family: Bach
    given: Francis
  issued:
  - year: '2016'
    month: '5'
  title: Minimizing Finite Sums with the Stochastic Average Gradient
  note: 'citecount: 00250'
  URL: https://hal.inria.fr/hal-00860051

- id: vidal_subspace_2011
  type: article-journal
  author:
  - family: Vidal
    given: R.
  issued:
  - year: '2011'
    month: '3'
  title: Subspace Clustering
  container-title: IEEE Signal Processing Magazine
  page: '52-68'
  volume: '28'
  issue: '2'
  note: 'citecount: 00774'
  abstract: Over the past few decades, significant progress has been made in clustering
    high-dimensional data sets distributed around a collection of linear and affine
    subspaces. This article presented a review of such progress, which included a
    number of existing subspace clustering algorithms together with an experimental
    evaluation on the motion segmentation and face clustering problems in computer
    vision.
  DOI: 10.1109/MSP.2010.939739
  ISSN: '1053-5888'

- id: wang_general_2004
  type: article-journal
  author:
  - family: Wang
    given: Shuning
  issued:
  - year: '2004'
    month: '9'
  title: General constructive representations for continuous piecewise-linear functions
  container-title: 'IEEE Transactions on Circuits and Systems I: Regular Papers'
  page: '1889-1896'
  volume: '51'
  issue: '9'
  note: 'citecount: 00014'
  abstract: The problem of constructing a canonical representation for an arbitrary
    continuous piecewise-linear (PWL) function in any dimension is considered in this
    paper. We solve the problem based on a general lattice PWL representation, which
    can be determined for a given continuous PWL function using existing methods.
    We first transform the lattice PWL representation into the difference of two convex
    functions, then propose a constructive procedure to rewrite the latter as a canonical
    representation that consists of at most n-level nestings of absolute-value functions
    in n dimensions, hence give a thorough solution to the problem mentioned above.
    In addition, we point out that there exist notable differences between a lattice
    representation and the two novel general constructive representations proposed
    in this paper, and explain that these differences make all the three representations
    be of their particular interests.
  DOI: 10.1109/TCSI.2004.834521
  ISSN: '1549-8328'

- id: pinkus_approximation_1999
  type: article-journal
  author:
  - family: Pinkus
    given: Allan
  issued:
  - year: '1999'
  accessed:
  - year: '2017'
    month: '5'
    day: '29'
  title: Approximation theory of the MLP model in neural networks
  container-title: Acta Numerica
  page: '143-195'
  volume: '8'
  note: 'citecount: 00333'
  URL: http://journals.cambridge.org/abstract_s0962492900002919

- id: poggio_why_2016
  type: article-journal
  author:
  - family: Poggio
    given: Tomaso
  - family: Mhaskar
    given: Hrushikesh
  - family: Rosasco
    given: Lorenzo
  - family: Miranda
    given: Brando
  - family: Liao
    given: Qianli
  issued:
  - year: '2016'
    month: '11'
  accessed:
  - year: '2017'
    month: '4'
    day: '7'
  title: 'Why and When Can Deep – but Not Shallow – Networks Avoid the Curse of
    Dimensionality: A Review'
  title-short: Why and When Can Deep – but Not Shallow – Networks Avoid the Curse
    of Dimensionality
  container-title: arXiv:1611.00740 \[cs\]
  page: '26'
  note: 'citecount: 00002 arXiv: 1611.00740'
  abstract: The paper characterizes classes of functions for which deep learning can
    be exponentially better than shallow learning. Deep convolutional networks are
    a special case of these conditions, though weight sharing is not the main reason
    for their exponential advantage.
  URL: http://arxiv.org/abs/1611.00740

- id: wager_dropout_2013
  type: chapter
  author:
  - family: Wager
    given: Stefan
  - family: Wang
    given: Sida
  - family: Liang
    given: Percy
  editor:
  - family: Burges
    given: C. J. C.
  - family: Bottou
    given: L.
  - family: Welling
    given: M.
  - family: Ghahramani
    given: Z.
  - family: Weinberger
    given: K. Q.
  issued:
  - year: '2013'
  title: Dropout training as adaptive regularization
  container-title: Advances in Neural Information Processing Systems 26
  page: '351-359'
  note: 'citecount: 00156'
  URL: http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization

- id: duchi_adaptive_2011
  type: article-journal
  author:
  - family: Duchi
    given: John
  - family: Hazan
    given: Elad
  - family: Singer
    given: Yoram
  issued:
  - year: '2011'
  accessed:
  - year: '2016'
    month: '7'
    day: '31'
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
  container-title: Journal of Machine Learning Research
  page: '2121-2159'
  volume: '12'
  issue: Jul
  note: 'citecount: 01743'
  URL: http://www.jmlr.org/papers/v12/duchi11a.html
  ISSN: ISSN 1533-7928

- id: bishop_training_1995
  type: article-journal
  author:
  - family: Bishop
    given: Chris M
  issued:
  - year: '1995'
  title: Training with noise is equivalent to Tikhonov regularization
  container-title: Neural computation
  page: '108-116'
  volume: '7'
  issue: '1'
  note: 'citecount: 00545'

- id: hardt_identity_2016
  type: article-journal
  author:
  - family: Hardt
    given: Moritz
  - family: Ma
    given: Tengyu
  issued:
  - year: '2016'
    month: '11'
  title: Identity matters in Deep Learning
  container-title: arXiv:1611.04231 \[cs, stat\]
  note: 'citecount: 00001 arXiv: 1611.04231'
  abstract: An emerging design principle in deep learning is that each layer of a
    deep artificial neural network should be able to easily express the identity transformation.
    This idea not only motivated various normalization techniques, such as \\emph{batch
    normalization}, but was also key to the immense success of \\emph{residual networks}.
    In this work, we put the principle of \\emph{identity parameterization} on a more
    solid theoretical footing alongside further empirical progress. We first give
    a strikingly simple proof that arbitrarily deep linear residual networks have
    no spurious local optima. The same result for linear feed-forward networks in
    their standard parameterization is substantially more delicate. Second, we show
    that residual networks with ReLu activations have universal finite-sample expressivity
    in the sense that the network can represent any function of its sample provided
    that the model has more parameters than the sample size. Directly inspired by
    our theory, we experiment with a radically simple residual architecture consisting
    of only residual convolutional layers and ReLu activations, but no batch normalization,
    dropout, or max pool. Our model improves significantly on previous all-convolutional
    networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.
  URL: http://arxiv.org/abs/1611.04231

- id: sutskever_importance_2013
  type: paper-conference
  author:
  - family: Sutskever
    given: Ilya
  - family: Martens
    given: James
  - family: Dahl
    given: George
  - family: Hinton
    given: Geoffrey E.
  issued:
  - year: '2013'
    month: '2'
  accessed:
  - year: '2017'
    month: '6'
    day: '1'
  title: On the importance of initialization and momentum in deep learning
  container-title: PMLR
  page: '1139-1147'
  note: 'citecount: 00593'
  abstract: Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful
    models that were considered to be almost impossible to train using stochastic
    gradient descent with momentum. In this pa...
  URL: http://proceedings.mlr.press/v28/sutskever13.html

- id: ioffe_batch_2015
  type: article-journal
  author:
  - family: Ioffe
    given: Sergey
  - family: Szegedy
    given: Christian
  issued:
  - year: '2015'
    month: '2'
  accessed:
  - year: '2017'
    month: '4'
    day: '18'
  title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift'
  title-short: Batch Normalization
  container-title: arXiv:1502.03167 \[cs\]
  page: '11'
  note: 'citecount: 01618 arXiv: 1502.03167'
  abstract: 'Training Deep Neural Networks is complicated by the fact that the distribution
    of each layer’s inputs changes during training, as the parameters of the previous
    layers change. This slows down the training by requiring lower learning rates
    and careful parameter initialization, and makes it notoriously hard to train models
    with saturating nonlinearities. We refer to this phenomenon as internal covariate
    shift, and address the problem by normalizing layer inputs. Our method draws its
    strength from making normalization a part of the model architecture and performing
    the normalization for each training mini-batch. Batch Normalization allows us
    to use much higher learning rates and be less careful about initialization. It
    also acts as a regularizer, in some cases eliminating the need for Dropout. Applied
    to a state-of-the-art image classification model, Batch Normalization achieves
    the same accuracy with 14 times fewer training steps, and beats the original model
    by a significant margin. Using an ensemble of batch-normalized networks, we improve
    upon the best published result on ImageNet classification: reaching 4.9% top-5
    validation error (and 4.8% test error), exceeding the accuracy of human raters.'
  URL: http://arxiv.org/abs/1502.03167

- id: su_differential_2016
  type: article-journal
  author:
  - family: Su
    given: Weijie
  - family: Boyd
    given: Stephen
  - family: Candès
    given: Emmanuel J.
  issued:
  - year: '2016'
  accessed:
  - year: '2017'
    month: '5'
    day: '31'
  title: 'A differential equation for modeling Nesterov’s accelerated gradient method:
    Theory and insights'
  title-short: A Differential Equation for Modeling Nesterov’s Accelerated Gradient
    Method
  container-title: Journal of Machine Learning Research
  page: '1-43'
  volume: '17'
  issue: '153'
  note: 'citecount: 00083 arXiv: 1503.01243'
  URL: http://jmlr.org/papers/v17/15-084.html

- id: nesterov_introductory_2004
  type: book
  author:
  - family: Nesterov
    given: Yurii
  issued:
  - year: '2004'
  accessed:
  - year: '2017'
    month: '6'
    day: '3'
  title: Introductory Lectures on Convex Optimization - A Basic Course
  collection-title: Applied Optimization
  collection-number: '87'
  publisher: Springer
  note: 'citecount: 02412'
  abstract: It was in the middle of the 1980s, when the seminal paper by Kar­ markar
    opened a new epoch in nonlinear optimization. The importance of this paper,...
  URL: http://www.springer.com/de/book/9781402075537
  ISBN: '978-1-4613-4691-3'

- id: jordan_gradient-based_2017
  type: no-type
  author:
  - family: Jordan
    given: Michael
  issued:
  - year: '2017'
    month: '5'
  accessed:
  - year: '2017'
    month: '5'
    day: '31'
  title: 'On Gradient-Based Optimization: Accelerated, Distributed, Asynchronous
    and Stochastic'
  title-short: On Gradient-Based Optimization
  publisher-place: Simons Institute
  genre: Talk
  note: 'citecount: 00000 youtube: https://youtu.be/VE2ITg\_hGnI duration: 3725
    seconds'
  abstract: 'Many new theoretical challenges have arisen in the area of gradient-based
    optimization for large-scale statistical data analysis, driven by the needs of
    applications and the opportunities provided by new hardware and software platforms.
    I discuss several recent results in this area, including: (1) a new framework
    for understanding Nesterov acceleration, obtained by taking a continuous-time,
    Lagrangian/Hamiltonian perspective, (2) a general theory of asynchronous optimization
    in multi-processor systems, (3) a computationally-efficient approach to variance
    reduction, and (4) a primal-dual methodology for gradient-based optimization that
    targets communication bottlenecks in distributed systems.'
  URL: https://simons.berkeley.edu/talks/michael-jordan-2017-5-2

- id: nesterov_method_1983
  type: article-journal
  author:
  - family: Nesterov
    given: Yurii
  issued:
  - year: '1983'
  title: A method of solving a convex programming problem with convergence rate O
    (1/k2)
  container-title: Soviet Mathematics Doklady
  page: '372-376'
  volume: '27'
  note: 'citecount: 01269'
  URL: http://www.core.ucl.ac.be/~nesterov/Research/Papers/DAN83.pdf

- id: saxe_exact_2013
  type: article-journal
  author:
  - family: Saxe
    given: Andrew M.
  - family: McClelland
    given: James L.
  - family: Ganguli
    given: Surya
  issued:
  - year: '2013'
    month: '12'
  title: Exact solutions to the nonlinear dynamics of learning in deep linear neural
    networks
  container-title: arXiv:1312.6120 \[cond-mat, q-bio, stat\]
  note: 'citecount: 00174 arXiv: 1312.6120'
  abstract: 'Despite the widespread practical success of deep learning methods,
    our theoretical understanding of the dynamics of learning in deep neural networks
    remains quite sparse. We attempt to bridge the gap between the theory and practice
    of deep learning by systematically analyzing learning dynamics for the restricted
    case of deep linear neural networks. Despite the linearity of their input-output
    map, such networks have nonlinear gradient descent dynamics on weights that change
    with the addition of each new hidden layer. We show that deep linear networks
    exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear
    networks, including long plateaus followed by rapid transitions to lower error
    solutions, and faster convergence from greedy unsupervised pretraining initial
    conditions than from random initial conditions. We provide an analytical description
    of these phenomena by finding new exact solutions to the nonlinear dynamics of
    deep learning. Our theoretical analysis also reveals the surprising finding that
    as the depth of a network approaches infinity, learning speed can nevertheless
    remain finite: for a special class of initial conditions on the weights, very
    deep networks incur only a finite, depth independent, delay in learning speed
    relative to shallow networks. We show that, under certain conditions on the training
    data, unsupervised pretraining can find this special class of initial conditions,
    while scaled random Gaussian initializations cannot. We further exhibit a new
    class of random orthogonal initial conditions on weights that, like unsupervised
    pre-training, enjoys depth independent learning times. We further show that these
    initial conditions also lead to faithful propagation of gradients even in deep
    nonlinear networks, as long as they operate in a special regime known as the edge
    of chaos.'
  URL: http://arxiv.org/abs/1312.6120

- id: montufar_number_2014
  type: chapter
  author:
  - family: Montufar
    given: Guido F
  - family: Pascanu
    given: Razvan
  - family: Cho
    given: Kyunghyun
  - family: Bengio
    given: Yoshua
  editor:
  - family: Ghahramani
    given: Z.
  - family: Welling
    given: M.
  - family: Cortes
    given: C.
  - family: Lawrence
    given: N. D.
  - family: Weinberger
    given: K. Q.
  issued:
  - year: '2014'
  title: On the Number of Linear Regions of Deep Neural Networks
  container-title: Advances in Neural Information Processing Systems 27
  publisher: Curran Associates, Inc.
  page: '2924-2932'
  note: 'citecount: 00130'
  URL: http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf

- id: bruna_community_2017
  type: article-journal
  author:
  - family: Bruna
    given: Joan
  - family: Li
    given: Xiang
  issued:
  - year: '2017'
    month: '5'
  accessed:
  - year: '2017'
    month: '6'
    day: '11'
  title: Community Detection with Graph Neural Networks
  container-title: arXiv:1705.08415 \[stat\]
  note: 'citecount: 00000 arXiv: 1705.08415'
  abstract: We study data-driven methods for community detection in graphs. This estimation
    problem is typically formulated in terms of the spectrum of certain operators,
    as well as via posterior inference under certain probabilistic graphical models.
    Focusing on random graph families such as the Stochastic Block Model, recent research
    has unified these two approaches, and identified both statistical and computational
    signal-to-noise detection thresholds. We embed the resulting class of algorithms
    within a generic family of graph neural networks and show that they can reach
    those detection thresholds in a purely data-driven manner, without access to the
    underlying generative models and with no parameter assumptions. The resulting
    model is also tested on real datasets, requiring less computational steps and
    performing significantly better than rigid parametric models.
  keyword: ml, ml/nn, networks/communities, networks/sbm, wf:will\_assess
  URL: http://arxiv.org/abs/1705.08415

- id: tresp_learning_2015
  type: article-journal
  author:
  - family: Tresp
    given: Volker
  - family: Esteban
    given: Cristóbal
  - family: Yang
    given: Yinchong
  - family: Baier
    given: Stephan
  - family: Krompaß
    given: Denis
  issued:
  - year: '2015'
    month: '11'
  accessed:
  - year: '2017'
    month: '6'
    day: '11'
  title: Learning with Memory Embeddings
  container-title: arXiv:1511.07972 \[cs\]
  note: 'citecount: 00004 arXiv: 1511.07972'
  abstract: Embedding learning, a.k.a. representation learning, has been shown to
    be able to model large-scale semantic knowledge graphs. A key concept is a mapping
    of the knowledge graph to a tensor representation whose entries are predicted
    by models using latent representations of generalized entities. Latent variable
    models are well suited to deal with the high dimensionality and sparsity of typical
    knowledge graphs. In recent publications the embedding models were extended to
    also consider time evolutions, time patterns and subsymbolic representations.
    In this paper we map embedding models, which were developed purely as solutions
    to technical problems for modelling temporal knowledge graphs, to various cognitive
    memory functions, in particular to semantic and concept memory, episodic memory,
    sensory memory, short-term memory, and working memory. We discuss learning, query
    answering, the path from sensory input to semantic decoding, and the relationship
    between episodic memory and semantic memory. We introduce a number of hypotheses
    on human memory that can be derived from the developed mathematical models.
  keyword: ml/embeddings/tensor, wf:will\_assess
  URL: http://arxiv.org/abs/1511.07972

- id: bousquet_advanced_2004
  type: book
  editor:
  - family: Bousquet
    given: Olivier
  - family: Luxburg
    given: Ulrike
    dropping-particle: von
  - family: Rätsch
    given: Gunnar
  issued:
  - year: '2004'
  accessed:
  - year: '2016'
    month: '7'
    day: '25'
  title: Advanced Lectures on Machine Learning
  collection-title: Lecture Notes in Computer Science
  publisher: Springer Berlin Heidelberg
  publisher-place: Berlin, Heidelberg
  volume: '3176'
  note: 'citecount: 00036'
  URL: http://link.springer.com/10.1007/b100712
  ISBN: 978-3-540-23122-6 978-3-540-28650-9

- id: pascanu_number_2013
  type: article-journal
  author:
  - family: Pascanu
    given: Razvan
  - family: Montufar
    given: Guido
  - family: Bengio
    given: Yoshua
  issued:
  - year: '2013'
    month: '12'
  title: On the number of response regions of deep feed forward networks with piece-wise
    linear activations
  container-title: arXiv:1312.6098 \[cs\]
  note: 'citecount: 00032 arXiv: 1312.6098'
  abstract: This paper explores the complexity of deep feedforward networks with linear
    pre-synaptic couplings and rectified linear activations. This is a contribution
    to the growing body of work contrasting the representational power of deep and
    shallow network architectures. In particular, we offer a framework for comparing
    deep and shallow models that belong to the family of piecewise linear functions
    based on computational geometry. We look at a deep rectifier multi-layer perceptron
    (MLP) with linear outputs units and compare it with a single layer version of
    the model. In the asymptotic regime, when the number of inputs stays constant,
    if the shallow model has \$kn\$ hidden units and \$n\_0\$ inputs, then the number
    of linear regions is \$O(k\^{n\_0}n\^{n\_0})\$. For a \$k\$ layer model with \$n\$
    hidden units on each layer it is \$\\Omega(\\left\\lfloor {n}/{n\_0}\\right\\rfloor\^{k-1}n\^{n\_0})\$.
    The number \$\\left\\lfloor{n}/{n\_0}\\right\\rfloor\^{k-1}\$ grows faster than
    \$k\^{n\_0}\$ when \$n\$ tends to infinity or when \$k\$ tends to infinity and
    \$n \\geq 2n\_0\$. Additionally, even when \$k\$ is small, if we restrict \$n\$
    to be \$2n\_0\$, we can show that a deep model has considerably more linear regions
    that a shallow one. We consider this as a first step towards understanding the
    complexity of these models and specifically towards providing suitable mathematical
    tools for future analysis.
  URL: http://arxiv.org/abs/1312.6098
...
