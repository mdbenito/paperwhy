---
references:
- id: zagoruyko_diracnets:_2017
  type: article-journal
  author:
  - family: Zagoruyko
    given: Sergey
  - family: Komodakis
    given: Nikos
  issued:
  - year: '2017'
    month: '6'
  title: 'DiracNets: Training Very Deep Neural Networks Without Skip-Connections'
  title-short: DiracNets
  container-title: arXiv:1706.00388 \[cs\]
  note: 'arXiv: 1706.00388'
  abstract: Deep neural networks with skip-connections, such as ResNet, show excellent
    performance in various image classification benchmarks. It is though observed
    that the initial motivation behind them - training deeper networks - does not
    actually hold true, and the benefits come from increased capacity, rather than
    from depth. Motivated by this, and inspired from ResNet, we propose a simple Dirac
    weight parameterization, which allows us to train very deep plain networks without
    skip-connections, and achieve nearly the same performance. This parameterization
    has a minor computational cost at training time and no cost at all at inference.
    We’re able to achieve 95.5% accuracy on CIFAR-10 with 34-layer deep plain network,
    surpassing 1001-layer deep ResNet, and approaching Wide ResNet. Our parameterization
    also mostly eliminates the need of careful initialization in residual and non-residual
    networks. The code and models for our experiments are available at https://github.com/szagoruyko/diracnets
  URL: http://arxiv.org/abs/1706.00388

- id: dauphin_identifying_2014
  type: chapter
  author:
  - family: Dauphin
    given: Yann N
  - family: Pascanu
    given: Razvan
  - family: Gulcehre
    given: Caglar
  - family: Cho
    given: Kyunghyun
  - family: Ganguli
    given: Surya
  - family: Bengio
    given: Yoshua
  editor:
  - family: Ghahramani
    given: Z.
  - family: Welling
    given: M.
  - family: Cortes
    given: C.
  - family: Lawrence
    given: N. D.
  - family: Weinberger
    given: K. Q.
  issued:
  - year: '2014'
  title: Identifying and attacking the saddle point problem in high-dimensional non-convex
    optimization
  container-title: Advances in Neural Information Processing Systems 27
  publisher: Curran Associates, Inc.
  page: '2933-2941'
  note: 'citecount: 00221 arXiv: 1405.4604'
  URL: http://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf

- id: goodfellow_qualitatively_2014
  type: article-journal
  author:
  - family: Goodfellow
    given: Ian J.
  - family: Vinyals
    given: Oriol
  - family: Saxe
    given: Andrew M.
  issued:
  - year: '2014'
    month: '12'
  accessed:
  - year: '2015'
    month: '3'
    day: '31'
  title: Qualitatively characterizing neural network optimization problems
  container-title: arXiv:1412.6544 \[cs, stat\]
  page: '11'
  note: 'citecount: 00032 arXiv: 1412.6544'
  abstract: Training neural networks involves solving large-scale non-convex optimization
    problems. This task has long been believed to be extremely difficult, with fear
    of local minima and other obstacles motivating a variety of schemes to improve
    optimization, such as unsupervised pretraining. However, modern neural networks
    are able to achieve negligible training error on complex tasks, using only direct
    training with stochastic gradient descent. We introduce a simple analysis technique
    to look for evidence that such networks are overcoming local optima. We find that,
    in fact, on a straight path from initialization to solution, a variety of state
    of the art neural networks never encounter any significant obstacles.
  keyword: priority, queue
  URL: http://arxiv.org/abs/1412.6544

- id: he_identity_2016
  type: article-journal
  author:
  - family: He
    given: Kaiming
  - family: Zhang
    given: Xiangyu
  - family: Ren
    given: Shaoqing
  - family: Sun
    given: Jian
  issued:
  - year: '2016'
    month: '3'
  accessed:
  - year: '2017'
    month: '5'
    day: '2'
  title: Identity Mappings in Deep Residual Networks
  container-title: arXiv:1603.05027 \[cs\]
  page: '15'
  note: 'citecount: 00287 arXiv: 1603.05027'
  abstract: 'Deep residual networks have emerged as a family of extremely deep architectures
    showing compelling accuracy and nice convergence behaviors. In this paper, we
    analyze the propagation formulations behind the residual building blocks, which
    suggest that the forward and backward signals can be directly propagated from
    one block to any other block, when using identity mappings as the skip connections
    and after-addition activation. A series of ablation experiments support the importance
    of these identity mappings. This motivates us to propose a new residual unit,
    which makes training easier and improves generalization. We report improved results
    using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer
    ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers'
  keyword: queue, priority
  URL: http://arxiv.org/abs/1603.05027

- id: he_deep_2016
  type: paper-conference
  author:
  - family: He
    given: Kaiming
  - family: Zhang
    given: Xiangyu
  - family: Ren
    given: Shaoqing
  - family: Sun
    given: Jian
  issued:
  - year: '2016'
  accessed:
  - year: '2017'
    month: '5'
    day: '2'
  title: Deep Residual Learning for Image Recognition
  page: '770-778'
  note: 'citecount: 02451 Extended version with appendix from the Arxiv'
  abstract: Deeper neural networks are more difficult to train. We present a residual
    learning framework to ease the training of networks that are substantially deeper
    than those used previously. We explicitly reformulate the layers as learning residual
    functions with reference to the layer inputs, instead of learning unreferenced
    functions. We provide comprehensive empirical evidence showing that these residual
    networks are easier to optimize, and can gain accuracy from considerably increased
    depth. On the ImageNet dataset we evaluate residual nets with a depth of up to
    152 layers—8x deeper than VGG nets but still having lower complexity. An ensemble
    of these residual nets achieves 3.57% error on the ImageNet test set. This result
    won the 1st place on the ILSVRC 2015 classification task. We also present analysis
    on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central
    importance for many visual recognition tasks. Solely due to our extremely deep
    representations, we obtain a 28% relative improvement on the COCO object detection
    dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO
    2015 competitions, where we also won the 1st places on the tasks of ImageNet detection,
    ImageNet localization, COCO detection, and COCO segmentation.
  URL: http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html

- id: montufar_number_2014
  type: chapter
  author:
  - family: Montufar
    given: Guido F
  - family: Pascanu
    given: Razvan
  - family: Cho
    given: Kyunghyun
  - family: Bengio
    given: Yoshua
  editor:
  - family: Ghahramani
    given: Z.
  - family: Welling
    given: M.
  - family: Cortes
    given: C.
  - family: Lawrence
    given: N. D.
  - family: Weinberger
    given: K. Q.
  issued:
  - year: '2014'
  title: On the Number of Linear Regions of Deep Neural Networks
  container-title: Advances in Neural Information Processing Systems 27
  publisher: Curran Associates, Inc.
  page: '2924-2932'
  note: 'citecount: 00139'
  URL: http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf

- id: hardt_identity_2016
  type: article-journal
  author:
  - family: Hardt
    given: Moritz
  - family: Ma
    given: Tengyu
  issued:
  - year: '2016'
    month: '11'
  title: Identity matters in Deep Learning
  container-title: arXiv:1611.04231 \[cs, stat\]
  note: 'citecount: 00015 arXiv: 1611.04231'
  abstract: An emerging design principle in deep learning is that each layer of a
    deep artificial neural network should be able to easily express the identity transformation.
    This idea not only motivated various normalization techniques, such as \\emph{batch
    normalization}, but was also key to the immense success of \\emph{residual networks}.
    In this work, we put the principle of \\emph{identity parameterization} on a more
    solid theoretical footing alongside further empirical progress. We first give
    a strikingly simple proof that arbitrarily deep linear residual networks have
    no spurious local optima. The same result for linear feed-forward networks in
    their standard parameterization is substantially more delicate. Second, we show
    that residual networks with ReLu activations have universal finite-sample expressivity
    in the sense that the network can represent any function of its sample provided
    that the model has more parameters than the sample size. Directly inspired by
    our theory, we experiment with a radically simple residual architecture consisting
    of only residual convolutional layers and ReLu activations, but no batch normalization,
    dropout, or max pool. Our model improves significantly on previous all-convolutional
    networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.
  URL: http://arxiv.org/abs/1611.04231

- id: bishop_training_1995
  type: article-journal
  author:
  - family: Bishop
    given: Chris M
  issued:
  - year: '1995'
    month: '1'
  title: Training with noise is equivalent to Tikhonov regularization
  container-title: Neural computation
  page: '108-116'
  volume: '7'
  issue: '1'
  note: 'citecount: 00550'
  URL: http://dl.acm.org/citation.cfm?id=211185
  DOI: 10.1162/neco.1995.7.1.108

- id: bengio_greedy_2007
  type: chapter
  author:
  - family: Bengio
    given: Yoshua
  - family: Lamblin
    given: Pascal
  - family: Popovici
    given: Dan
  - family: Larochelle
    given: Hugo
  editor:
  - family: Schölkopf
    given: P. B.
  - family: Platt
    given: J. C.
  - family: Hoffman
    given: T.
  issued:
  - year: '2007'
  accessed:
  - year: '2017'
    month: '5'
    day: '13'
  title: Greedy layer-wise training of Deep Networks
  container-title: Advances in Neural Information Processing Systems 19
  publisher: MIT Press
  page: '153-160'
  note: 'citecount: 02230'
  URL: http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf

- id: hinton_improving_2012
  type: article-journal
  author:
  - family: Hinton
    given: Geoffrey E.
  - family: Srivastava
    given: Nitish
  - family: Krizhevsky
    given: Alex
  - family: Sutskever
    given: Ilya
  - family: Salakhutdinov
    given: Ruslan R.
  issued:
  - year: '2012'
    month: '7'
  accessed:
  - year: '2017'
    month: '4'
    day: '28'
  title: Improving neural networks by preventing co-adaptation of feature detectors
  container-title: arXiv:1207.0580 \[cs\]
  page: '18'
  note: 'citecount: 01928 arXiv: 1207.0580'
  abstract: When a large feedforward neural network is trained on a small training
    set, it typically performs poorly on held-out test data. This “overfitting” is
    greatly reduced by randomly omitting half of the feature detectors on each training
    case. This prevents complex co-adaptations in which a feature detector is only
    helpful in the context of several other specific feature detectors. Instead, each
    neuron learns to detect a feature that is generally helpful for producing the
    correct answer given the combinatorially large variety of internal contexts in
    which it must operate. Random “dropout” gives big improvements on many benchmark
    tasks and sets new records for speech and object recognition.
  URL: http://arxiv.org/abs/1207.0580

- id: goodfellow_maxout_2013
  type: article-journal
  author:
  - family: Goodfellow
    given: Ian J.
  - family: Warde-Farley
    given: David
  - family: Mirza
    given: Mehdi
  - family: Courville
    given: Aaron
  - family: Bengio
    given: Yoshua
  issued:
  - year: '2013'
    month: '2'
  accessed:
  - year: '2017'
    month: '4'
    day: '18'
  title: Maxout Networks
  container-title: arXiv:1302.4389 \[cs, stat\]
  page: '9'
  note: 'citecount: 00781 code: http://www-etud.iro.umontreal.ca/\~goodfeli/maxout.html
    arxiv: 1302.4389'
  abstract: 'We consider the problem of designing models to leverage a recently
    introduced approximate model averaging technique called dropout. We define a simple
    new model called maxout (so named because its output is the max of a set of inputs,
    and because it is a natural companion to dropout) designed to both facilitate
    optimization by dropout and improve the accuracy of dropout’s fast approximate
    model averaging technique. We empirically verify that the model successfully accomplishes
    both of these tasks. We use maxout and dropout to demonstrate state of the art
    classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100,
    and SVHN.'
  URL: http://arxiv.org/abs/1302.4389

- id: glorot_deep_2011
  type: paper-conference
  author:
  - family: Glorot
    given: Xavier
  - family: Bordes
    given: Antoine
  - family: Bengio
    given: Yoshua
  issued:
  - year: '2011'
    month: '4'
  accessed:
  - year: '2017'
    month: '4'
    day: '7'
  title: Deep Sparse Rectifier Neural Networks
  container-title: Proceedings of the Fourteenth International Conference on Artificial
    Intelligence and Statistics April 11-13, 2011, Fort Lauderdale, FL, USA
  publisher-place: Florida, USA
  page: '315-323'
  volume: '15'
  note: 'citecount: 00995'
  abstract: While logistic sigmoid neurons are more biologically plausible than hyperbolic
    tangent neurons, the latter work better for training multi-layer neural networks.
    This paper shows that rectifying neurons are an even better model of biological
    neurons and yield equal or better performance than hyperbolic tangent networks
    in spite of the hard non-linearity and non-differentiability at zero, creating
    sparse representations with true zeros which seem remarkably suitable for naturally
    sparse data. Even though they can take advantage of semi-supervised setups with
    extra-unlabeled data, deep rectifier networks can reach their best performance
    without requiring any unsupervised pre-training on purely supervised tasks with
    large labeled datasets. Hence, these results can be seen as a new milestone in
    the attempts at understanding the difficulty in training deep but purely supervised
    neural networks, and closing the performance gap between neural networks learnt
    with and without unsupervised pre-training.
  URL: http://www.jmlr.org/proceedings/papers/v15/

- id: poggio_why_2016
  type: article-journal
  author:
  - family: Poggio
    given: Tomaso
  - family: Mhaskar
    given: Hrushikesh
  - family: Rosasco
    given: Lorenzo
  - family: Miranda
    given: Brando
  - family: Liao
    given: Qianli
  issued:
  - year: '2016'
    month: '11'
  accessed:
  - year: '2017'
    month: '4'
    day: '7'
  title: 'Why and When Can Deep – but Not Shallow – Networks Avoid the Curse of
    Dimensionality: A Review'
  title-short: Why and When Can Deep – but Not Shallow – Networks Avoid the Curse
    of Dimensionality
  container-title: arXiv:1611.00740 \[cs\]
  page: '26'
  note: 'citecount: 00004 arXiv: 1611.00740'
  abstract: The paper characterizes classes of functions for which deep learning can
    be exponentially better than shallow learning. Deep convolutional networks are
    a special case of these conditions, though weight sharing is not the main reason
    for their exponential advantage.
  URL: http://arxiv.org/abs/1611.00740

- id: lin_why_2016
  type: article-journal
  author:
  - family: Lin
    given: Henry W.
  - family: Tegmark
    given: Max
  issued:
  - year: '2016'
    month: '8'
  accessed:
  - year: '2016'
    month: '10'
    day: '26'
  title: Why does deep and cheap learning work so well?
  container-title: arXiv:1608.08225 \[cond-mat, stat\]
  page: '17'
  note: 'citecount: 00022 arXiv: 1608.08225'
  abstract: 'We show how the success of deep learning depends not only on mathematics
    but also on physics: although well-known mathematical theorems guarantee that
    neural networks can approximate arbitrary functions well, the class of functions
    of practical interest can be approximated through “cheap learning” with exponentially
    fewer parameters than generic ones, because they have simplifying properties tracing
    back to the laws of physics. The exceptional simplicity of physics-based functions
    hinges on properties such as symmetry, locality, compositionality and polynomial
    log-probability, and we explore how these properties translate into exceptionally
    simple neural networks approximating both natural phenomena such as images and
    abstract representations thereof such as drawings. We further argue that when
    the statistical process generating the data is of a certain hierarchical form
    prevalent in physics and machine-learning, a deep neural network can be more efficient
    than a shallow one. We formalize these claims using information theory and discuss
    the relation to renormalization group procedures. We prove various “no-flattening
    theorems” showing when such efficient deep networks cannot be accurately approximated
    by shallow ones without efficiency loss: flattening even linear functions can
    be costly, and flattening polynomials is exponentially expensive; we use group
    theoretic techniques to show that n variables cannot be multiplied using fewer
    than 2\^n neurons in a single hidden layer.'
  URL: http://arxiv.org/abs/1608.08225

- id: tang_deep_2013
  type: article-journal
  author:
  - family: Tang
    given: Yichuan
  issued:
  - year: '2013'
    month: '6'
  accessed:
  - year: '2015'
    month: '4'
    day: '29'
  title: Deep learning using linear Support Vector Machines
  container-title: arXiv:1306.0239 \[cs, stat\]
  page: '6'
  note: 'citecount: 00169 arXiv: 1306.0239'
  abstract: Recently, fully-connected and convolutional neural networks have been
    trained to achieve state-of-the-art performance on a wide variety of tasks such
    as speech recognition, image classification, natural language processing, and
    bioinformatics. For classification tasks, most of these “deep learning” models
    employ the softmax activation function for prediction and minimize cross-entropy
    loss. In this paper, we demonstrate a small but consistent advantage of replacing
    the softmax layer with a linear support vector machine. Learning minimizes a margin-based
    loss instead of the cross-entropy loss. While there have been various combinations
    of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply
    replacing softmax with linear SVMs gives significant gains on popular deep learning
    datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop’s
    face expression recognition challenge.
  URL: http://arxiv.org/abs/1306.0239

- id: baldi_understanding_2013
  type: chapter
  author:
  - family: Baldi
    given: Pierre
  - family: Sadowski
    given: Peter J
  editor:
  - family: Burges
    given: C. J. C.
  - family: Bottou
    given: L.
  - family: Welling
    given: M.
  - family: Ghahramani
    given: Z.
  - family: Weinberger
    given: K. Q.
  issued:
  - year: '2013'
  title: Understanding Dropout
  container-title: Advances in Neural Information Processing Systems 26
  page: '2814-2822'
  note: 'citecount: 00067'
  abstract: Dropout is a relatively new algorithm for training neural networks which
    relies on stochastically dropping out neurons during training in order to avoid
    the co-adaptation of feature detectors. We introduce a general formalism for studying
    dropout on either units or connections, with arbitrary probability values, and
    use it to analyze the averaging and regularizing properties of dropout in both
    linear and non-linear networks. For deep neural networks, the averaging properties
    of dropout are characterized by three recursive equations, including the approximation
    of expectations by normalized weighted geometric means. We provide estimates and
    bounds for these approximations and corroborate the results with simulations.
    We also show in simple cases how dropout performs stochastic gradient descent
    on a regularized error function.
  URL: http://papers.nips.cc/paper/4878-understanding-dropout.pdf

- id: wager_dropout_2013
  type: chapter
  author:
  - family: Wager
    given: Stefan
  - family: Wang
    given: Sida
  - family: Liang
    given: Percy
  editor:
  - family: Burges
    given: C. J. C.
  - family: Bottou
    given: L.
  - family: Welling
    given: M.
  - family: Ghahramani
    given: Z.
  - family: Weinberger
    given: K. Q.
  issued:
  - year: '2013'
  title: Dropout training as adaptive regularization
  container-title: Advances in Neural Information Processing Systems 26
  publisher: Curran Associates, Inc.
  page: '351-359'
  note: 'citecount: 00158'
  URL: http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf

- id: ba_multiple_2014
  type: article-journal
  author:
  - family: Ba
    given: Jimmy
  - family: Mnih
    given: Volodymyr
  - family: Kavukcuoglu
    given: Koray
  issued:
  - year: '2014'
    month: '12'
  title: Multiple Object Recognition with Visual Attention
  container-title: arXiv:1412.7755 \[cs\]
  note: 'citecount: 00167 arXiv: 1412.7755'
  abstract: We present an attention-based model for recognizing multiple objects in
    images. The proposed model is a deep recurrent neural network trained with reinforcement
    learning to attend to the most relevant regions of the input image. We show that
    the model learns to both localize and recognize multiple objects despite being
    given only class labels during training. We evaluate the model on the challenging
    task of transcribing house number sequences from Google Street View images and
    show that it is both more accurate than the state-of-the-art convolutional networks
    and uses fewer parameters and less computation.
  keyword: queue, priority
  URL: http://arxiv.org/abs/1412.7755

- id: helmbold_surprising_2016
  type: article-journal
  author:
  - family: Helmbold
    given: David P.
  - family: Long
    given: Philip M.
  issued:
  - year: '2016'
    month: '2'
  title: Surprising properties of dropout in deep networks
  container-title: arXiv:1602.04484 \[cs, math, stat\]
  page: '21'
  note: 'citecount: 00000 arXiv: 1602.04484'
  abstract: We analyze dropout in deep networks with rectified linear units and the
    quadratic loss. Our results expose surprising differences between the behavior
    of dropout and more traditional regularizers like weight decay. For example, on
    some simple data sets dropout training produces negative weights even though the
    output is the sum of the inputs. This provides a counterpoint to the suggestion
    that dropout discourages co-adaptation of weights. We also show that the dropout
    penalty can grow exponentially in the depth of the network while the weight-decay
    penalty remains essentially linear, and that dropout is insensitive to various
    re-scalings of the input features, outputs, and network weights. This last insensitivity
    implies that there are no isolated local minima of the dropout training criterion.
    Our work uncovers new properties of dropout, extends our understanding of why
    dropout succeeds, and lays the foundation for further progress.
  keyword: queue
  URL: http://arxiv.org/abs/1602.04484

- id: arias-castro_spectral_2017
  type: article-journal
  author:
  - family: Arias-Castro
    given: Ery
  - family: Lerman
    given: Gilad
  - family: Zhang
    given: Teng
  issued:
  - year: '2017'
  accessed:
  - year: '2017'
    month: '4'
    day: '12'
  title: Spectral Clustering based on local PCA
  container-title: Journal of Machine Learning Research
  page: '1-57'
  volume: '18'
  issue: '9'
  note: 'citecount: 00025 code: http://sciences.ucf.edu/math/tengz'
  URL: http://jmlr.org/papers/v18/14-318.html

- id: nevo_identifying_2017
  type: article-journal
  author:
  - family: Nevo
    given: Daniel
  - family: Ritov
    given: Ya’acov
  issued:
  - year: '2017'
  accessed:
  - year: '2017'
    month: '4'
    day: '16'
  title: Identifying a minimal class of models for high-dimensional data
  container-title: Journal of Machine Learning Research
  page: '1-29'
  volume: '18'
  issue: '24'
  note: 'citecount: 00000'
  URL: http://jmlr.org/papers/v18/16-172.html

- id: swirszcz_local_2016
  type: article-journal
  author:
  - family: Swirszcz
    given: Grzegorz
  - family: Czarnecki
    given: Wojciech Marian
  - family: Pascanu
    given: Razvan
  issued:
  - year: '2016'
    month: '11'
  accessed:
  - year: '2017'
    month: '5'
    day: '3'
  title: Local minima in training of neural networks
  container-title: arXiv:1611.06310 \[cs, stat\]
  page: '12'
  note: 'citecount: 00001 arXiv: 1611.06310'
  abstract: There has been a lot of recent interest in trying to characterize the
    error surface of deep models. This stems from a long standing question. Given
    that deep networks are highly nonlinear systems optimized by local gradient methods,
    why do they not seem to be affected by bad local minima? It is widely believed
    that training of deep models using gradient methods works so well because the
    error surface either has no local minima, or if they exist they need to be close
    in value to the global minimum. It is known that such results hold under very
    strong assumptions which are not satisfied by real models. In this paper we present
    examples showing that for such theorem to be true additional assumptions on the
    data, initialization schemes and/or the model classes have to be made. We look
    at the particular case of finite size datasets. We demonstrate that in this scenario
    one can construct counter-examples (datasets or initialization schemes) when the
    network does become susceptible to bad local minima over the weight space.
  URL: http://arxiv.org/abs/1611.06310

- id: bartlett_representational_2017
  type: no-type
  author:
  - family: Bartlett
    given: Peter
  issued:
  - year: '2017'
    month: '5'
  accessed:
  - year: '2017'
    month: '5'
    day: '2'
  title: Representational and optimization properties of Deep Residual Networks
  publisher-place: Simons Institute
  genre: Talk
  note: 'citecount: 00000 2681 seconds'
  URL: https://simons.berkeley.edu/talks/tba-1

- id: ioffe_batch_2015
  type: article-journal
  author:
  - family: Ioffe
    given: Sergey
  - family: Szegedy
    given: Christian
  issued:
  - year: '2015'
    month: '2'
  accessed:
  - year: '2017'
    month: '4'
    day: '18'
  title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift'
  title-short: Batch Normalization
  container-title: arXiv:1502.03167 \[cs\]
  page: '11'
  note: 'citecount: 01616 arXiv: 1502.03167'
  abstract: 'Training Deep Neural Networks is complicated by the fact that the distribution
    of each layer’s inputs changes during training, as the parameters of the previous
    layers change. This slows down the training by requiring lower learning rates
    and careful parameter initialization, and makes it notoriously hard to train models
    with saturating nonlinearities. We refer to this phenomenon as internal covariate
    shift, and address the problem by normalizing layer inputs. Our method draws its
    strength from making normalization a part of the model architecture and performing
    the normalization for each training mini-batch. Batch Normalization allows us
    to use much higher learning rates and be less careful about initialization. It
    also acts as a regularizer, in some cases eliminating the need for Dropout. Applied
    to a state-of-the-art image classification model, Batch Normalization achieves
    the same accuracy with 14 times fewer training steps, and beats the original model
    by a significant margin. Using an ensemble of batch-normalized networks, we improve
    upon the best published result on ImageNet classification: reaching 4.9% top-5
    validation error (and 4.8% test error), exceeding the accuracy of human raters.'
  URL: http://arxiv.org/abs/1502.03167

- id: jordan_gradient-based_2017
  type: no-type
  author:
  - family: Jordan
    given: Michael
  issued:
  - year: '2017'
    month: '5'
  accessed:
  - year: '2017'
    month: '5'
    day: '31'
  title: 'On Gradient-Based Optimization: Accelerated, Distributed, Asynchronous
    and Stochastic'
  title-short: On Gradient-Based Optimization
  publisher-place: Simons Institute
  genre: Talk
  note: 'citecount: 00000 youtube: https://youtu.be/VE2ITg\_hGnI duration: 3725
    seconds'
  abstract: 'Many new theoretical challenges have arisen in the area of gradient-based
    optimization for large-scale statistical data analysis, driven by the needs of
    applications and the opportunities provided by new hardware and software platforms.
    I discuss several recent results in this area, including: (1) a new framework
    for understanding Nesterov acceleration, obtained by taking a continuous-time,
    Lagrangian/Hamiltonian perspective, (2) a general theory of asynchronous optimization
    in multi-processor systems, (3) a computationally-efficient approach to variance
    reduction, and (4) a primal-dual methodology for gradient-based optimization that
    targets communication bottlenecks in distributed systems.'
  URL: https://simons.berkeley.edu/talks/michael-jordan-2017-5-2

- id: martius_extrapolation_2016
  type: article-journal
  author:
  - family: Martius
    given: Georg
  - family: Lampert
    given: Christoph H.
  issued:
  - year: '2016'
    month: '10'
  title: Extrapolation and learning equations
  container-title: arXiv:1610.02995 \[cs\]
  note: 'arXiv: 1610.02995'
  abstract: In classical machine learning, regression is treated as a black box process
    of identifying a suitable function from a hypothesis set without attempting to
    gain insight into the mechanism connecting inputs and outputs. In the natural
    sciences, however, finding an interpretable function for a phenomenon is the prime
    goal as it allows to understand and generalize results. This paper proposes a
    novel type of function learning network, called equation learner (EQL), that can
    learn analytical expressions and is able to extrapolate to unseen domains. It
    is implemented as an end-to-end differentiable feed-forward network and allows
    for efficient gradient based training. Due to sparsity regularization concise
    interpretable expressions can be obtained. Often the true underlying source expression
    is identified.
  URL: http://arxiv.org/abs/1610.02995
...
