
@article{zagoruyko_diracnets:_2017,
	title = {{DiracNets}: {Training} {Very} {Deep} {Neural} {Networks} {Without} {Skip}-{Connections}},
	shorttitle = {{DiracNets}},
	url = {http://arxiv.org/abs/1706.00388},
	abstract = {Deep neural networks with skip-connections, such as ResNet, show excellent performance in various image classification benchmarks. It is though observed that the initial motivation behind them - training deeper networks - does not actually hold true, and the benefits come from increased capacity, rather than from depth. Motivated by this, and inspired from ResNet, we propose a simple Dirac weight parameterization, which allows us to train very deep plain networks without skip-connections, and achieve nearly the same performance. This parameterization has a minor computational cost at training time and no cost at all at inference. We're able to achieve 95.5\% accuracy on CIFAR-10 with 34-layer deep plain network, surpassing 1001-layer deep ResNet, and approaching Wide ResNet. Our parameterization also mostly eliminates the need of careful initialization in residual and non-residual networks. The code and models for our experiments are available at https://github.com/szagoruyko/diracnets},
	journal = {arXiv:1706.00388 [cs]},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.00388},
	file = {Zagoruyko and Komodakis - 2017 - DiracNets Training Very Deep Neural Networks With.pdf:/home/miguel/Zotero/storage/U769HHCV/Zagoruyko and Komodakis - 2017 - DiracNets Training Very Deep Neural Networks With.pdf:application/pdf}
}

@incollection{dauphin_identifying_2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	url = {http://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	note = {citecount: 00221
arXiv: 1405.4604},
	pages = {2933--2941},
	file = {Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf:/home/miguel/Zotero/storage/XSK654W7/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf:application/pdf;Supplementary Material\: Attacking the saddle point problem.pdf:/home/miguel/Zotero/storage/4T9X892N/Supplementary Material Attacking the saddle point problem.pdf:application/pdf}
}

@article{goodfellow_qualitatively_2014,
	title = {Qualitatively characterizing neural network optimization problems},
	url = {http://arxiv.org/abs/1412.6544},
	abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
	urldate = {2015-03-31},
	journal = {arXiv:1412.6544 [cs, stat]},
	author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
	month = dec,
	year = {2014},
	note = {citecount: 00032 
arXiv: 1412.6544},
	keywords = {priority, queue},
	pages = {11},
	file = {Goodfellow and Vinyals - 2014 - Qualitatively characterizing neural network optimi.pdf:/home/miguel/Zotero/storage/3TZXHAKD/Goodfellow and Vinyals - 2014 - Qualitatively characterizing neural network optimi.pdf:application/pdf}
}

@article{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	language = {en},
	urldate = {2017-05-02},
	journal = {arXiv:1603.05027 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = mar,
	year = {2016},
	note = {citecount: 00287 
arXiv: 1603.05027},
	keywords = {queue, priority},
	pages = {15},
	file = {He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:/home/miguel/Zotero/storage/IP3MV4FK/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:application/pdf}
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	language = {en},
	urldate = {2017-05-02},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	note = {citecount: 02451 
Extended version with appendix from the Arxiv},
	pages = {770--778},
	file = {He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:/home/miguel/Zotero/storage/AW8V6EF4/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf;He et al. - 2016 - Deep Residual Learning for Image Recognition -- with appendix.pdf:/home/miguel/Zotero/storage/JZNFMFRC/He et al. - 2016 - Deep Residual Learning for Image Recognition -- with appendix.pdf:application/pdf}
}

@incollection{montufar_number_2014,
	title = {On the {Number} of {Linear} {Regions} of {Deep} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	note = {citecount: 00139},
	pages = {2924--2932},
	file = {Montufar et al. - 2014 - On the Number of Linear Regions of Deep Neural Net.pdf:/home/miguel/Zotero/storage/5Z5QTMIR/Montufar et al. - 2014 - On the Number of Linear Regions of Deep Neural Net.pdf:application/pdf;Montufar et al. - 2014 - On the Number of Linear Regions - Supplementary.pdf:/home/miguel/Zotero/storage/RN2SHB94/Montufar et al. - 2014 - On the Number of Linear Regions - Supplementary.pdf:application/pdf}
}

@article{hardt_identity_2016,
	title = {Identity matters in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1611.04231},
	abstract = {An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as {\textbackslash}emph\{batch normalization\}, but was also key to the immense success of {\textbackslash}emph\{residual networks\}. In this work, we put the principle of {\textbackslash}emph\{identity parameterization\} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.},
	journal = {arXiv:1611.04231 [cs, stat]},
	author = {Hardt, Moritz and Ma, Tengyu},
	month = nov,
	year = {2016},
	note = {citecount: 00015 
arXiv: 1611.04231},
	file = {Hardt and Ma - 2016 - Identity matters in Deep Learning.pdf:/home/miguel/Zotero/storage/9ZDX66AA/Hardt and Ma - 2016 - Identity matters in Deep Learning.pdf:application/pdf}
}

@article{bishop_training_1995,
	title = {Training with noise is equivalent to {Tikhonov} regularization},
	volume = {7},
	url = {http://dl.acm.org/citation.cfm?id=211185},
	doi = {10.1162/neco.1995.7.1.108},
	language = {en},
	number = {1},
	journal = {Neural computation},
	author = {Bishop, Chris M},
	month = jan,
	year = {1995},
	note = {citecount: 00550},
	pages = {108--116},
	file = {Bishop - 1995 - Training with noise is equivalent to Tikhonov regu.pdf:/home/miguel/Zotero/storage/69WRWDPR/Bishop - 1995 - Training with noise is equivalent to Tikhonov regu.pdf:application/pdf}
}

@incollection{bengio_greedy_2007,
	title = {Greedy layer-wise training of {Deep} {Networks}},
	url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
	urldate = {2017-05-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
	editor = {Schölkopf, P. B. and Platt, J. C. and Hoffman, T.},
	year = {2007},
	note = {citecount: 02230},
	pages = {153--160},
	file = {Bengio et al. - 2007 - Appendix - Greedy Layer-Wise Training of Deep Networks.pdf:/home/miguel/Zotero/storage/N3DIA4BS/Bengio et al. - 2007 - Appendix - Greedy Layer-Wise Training of Deep Networks.pdf:application/pdf;Bengio et al. - 2007 - Greedy Layer-Wise Training of Deep Networks.pdf:/home/miguel/Zotero/storage/26GIPMPM/Bengio et al. - 2007 - Greedy Layer-Wise Training of Deep Networks.pdf:application/pdf;comments - greedy layer-wise.tm:/home/miguel/Zotero/storage/7UZJB74D/comments - greedy layer-wise.tm:text/plain}
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	language = {en},
	urldate = {2017-04-28},
	journal = {arXiv:1207.0580 [cs]},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {citecount: 01928 
arXiv: 1207.0580},
	pages = {18},
	file = {Hinton et al. - 2012 - Improving neural networks by preventing co-adapta.pdf:/home/miguel/Zotero/storage/ABQ9S4VK/Hinton et al. - 2012 - Improving neural networks by preventing co-adapta.pdf:application/pdf;hinton_improving_2012.tm:/home/miguel/Zotero/storage/BJFSSF43/hinton_improving_2012.tm:text/plain}
}

@article{goodfellow_maxout_2013,
	title = {Maxout {Networks}},
	url = {http://arxiv.org/abs/1302.4389},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
	language = {en},
	urldate = {2017-04-18},
	journal = {arXiv:1302.4389 [cs, stat]},
	author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	month = feb,
	year = {2013},
	note = {citecount: 00781 
code: http://www-etud.iro.umontreal.ca/{\textasciitilde}goodfeli/maxout.html
arxiv: 1302.4389},
	pages = {9},
	file = {Goodfellow et al. - 2013 - Maxout Networks.pdf:/home/miguel/Zotero/storage/BUUR8VEJ/Goodfellow et al. - 2013 - Maxout Networks.pdf:application/pdf}
}

@inproceedings{glorot_deep_2011,
	address = {Florida, USA},
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	volume = {15},
	url = {http://www.jmlr.org/proceedings/papers/v15/},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
	language = {en},
	urldate = {2017-04-07},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}  {April} 11-13, 2011, {Fort} {Lauderdale}, {FL}, {USA}},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	month = apr,
	year = {2011},
	note = {citecount: 00995},
	pages = {315--323},
	file = {comments-relu.tm:/home/miguel/Zotero/storage/EWHXSB74/comments-relu.tm:text/plain;Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf:/home/miguel/Zotero/storage/AT7I648E/Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf:application/pdf}
}

@article{poggio_why_2016,
	title = {Why and {When} {Can} {Deep} -- but {Not} {Shallow} -- {Networks} {Avoid} the {Curse} of {Dimensionality}: a {Review}},
	shorttitle = {Why and {When} {Can} {Deep} -- but {Not} {Shallow} -- {Networks} {Avoid} the {Curse} of {Dimensionality}},
	url = {http://arxiv.org/abs/1611.00740},
	abstract = {The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning. Deep convolutional networks are a special case of these conditions, though weight sharing is not the main reason for their exponential advantage.},
	urldate = {2017-04-07},
	journal = {arXiv:1611.00740 [cs]},
	author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
	month = nov,
	year = {2016},
	note = {citecount: 00004 
arXiv: 1611.00740},
	pages = {26},
	file = {Poggio et al. - 2016 - Why and When Can Deep -- but Not Shallow -- Networ.pdf:/home/miguel/Zotero/storage/39X7RBW7/Poggio et al. - 2016 - Why and When Can Deep -- but Not Shallow -- Networ.pdf:application/pdf}
}

@article{lin_why_2016,
	title = {Why does deep and cheap learning work so well?},
	url = {http://arxiv.org/abs/1608.08225},
	abstract = {We show how the success of deep learning depends not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can be approximated through "cheap learning" with exponentially fewer parameters than generic ones, because they have simplifying properties tracing back to the laws of physics. The exceptional simplicity of physics-based functions hinges on properties such as symmetry, locality, compositionality and polynomial log-probability, and we explore how these properties translate into exceptionally simple neural networks approximating both natural phenomena such as images and abstract representations thereof such as drawings. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to renormalization group procedures. We prove various "no-flattening theorems" showing when such efficient deep networks cannot be accurately approximated by shallow ones without efficiency loss: flattening even linear functions can be costly, and flattening polynomials is exponentially expensive; we use group theoretic techniques to show that n variables cannot be multiplied using fewer than 2{\textasciicircum}n neurons in a single hidden layer.},
	language = {en},
	urldate = {2016-10-26},
	journal = {arXiv:1608.08225 [cond-mat, stat]},
	author = {Lin, Henry W. and Tegmark, Max},
	month = aug,
	year = {2016},
	note = {citecount: 00022 
arXiv: 1608.08225},
	pages = {17},
	file = {Lin and Tegmark - 2016 - Why does deep and cheap learning work so well.pdf:/home/miguel/Zotero/storage/SV79MM23/Lin and Tegmark - 2016 - Why does deep and cheap learning work so well.pdf:application/pdf;tegmark-comments.tm:/home/miguel/Zotero/storage/QD2MZHEG/tegmark-comments.tm:text/plain}
}

@article{tang_deep_2013,
	title = {Deep learning using linear {Support} {Vector} {Machines}},
	url = {http://arxiv.org/abs/1306.0239},
	abstract = {Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these "deep learning" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.},
	language = {en},
	urldate = {2015-04-29},
	journal = {arXiv:1306.0239 [cs, stat]},
	author = {Tang, Yichuan},
	month = jun,
	year = {2013},
	note = {citecount: 00169 
arXiv: 1306.0239},
	pages = {6},
	file = {comments-deep-svm.tm:/home/miguel/Zotero/storage/MEX4MJBV/comments-deep-svm.tm:text/plain;Tang - 2013 - Deep Learning using Linear Support Vector Machines copy.pdf:/home/miguel/Zotero/storage/SUTB6QJD/Tang - 2013 - Deep Learning using Linear Support Vector Machines copy.pdf:application/pdf}
}

@incollection{baldi_understanding_2013,
	title = {Understanding {Dropout}},
	url = {http://papers.nips.cc/paper/4878-understanding-dropout.pdf},
	abstract = {Dropout is a relatively new algorithm for training neural networks which relies on stochastically dropping out neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. We also show in simple cases how dropout performs stochastic gradient descent on a regularized error function.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	author = {Baldi, Pierre and Sadowski, Peter J},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	note = {citecount: 00067},
	pages = {2814--2822},
	file = {Baldi and Sadowski - 2013 - Understanding Dropout.pdf:/home/miguel/Zotero/storage/ZZEEDPWD/Baldi and Sadowski - 2013 - Understanding Dropout.pdf:application/pdf;comments-understanding-dropout.tm:/home/miguel/Zotero/storage/9B8ZSUH7/comments-understanding-dropout.tm:text/plain}
}

@incollection{wager_dropout_2013,
	title = {Dropout training as adaptive regularization},
	url = {http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Wager, Stefan and Wang, Sida and Liang, Percy},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	note = {citecount: 00158},
	pages = {351--359},
	file = {Appendix.pdf:/home/miguel/Zotero/storage/H2CJBFQN/Appendix.pdf:application/pdf;Wager et al. - 2013 - Dropout training as adaptive regularization.pdf:/home/miguel/Zotero/storage/KJ65IZK9/Wager et al. - 2013 - Dropout training as adaptive regularization.pdf:application/pdf}
}

@article{ba_multiple_2014,
	title = {Multiple {Object} {Recognition} with {Visual} {Attention}},
	url = {http://arxiv.org/abs/1412.7755},
	abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
	journal = {arXiv:1412.7755 [cs]},
	author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
	month = dec,
	year = {2014},
	note = {citecount: 00167 
arXiv: 1412.7755},
	keywords = {queue, priority},
	file = {Ba et al. - 2014 - Multiple Object Recognition with Visual Attention.pdf:/home/miguel/Zotero/storage/DGQ92QAX/Ba et al. - 2014 - Multiple Object Recognition with Visual Attention.pdf:application/pdf}
}

@article{helmbold_surprising_2016,
	title = {Surprising properties of dropout in deep networks},
	url = {http://arxiv.org/abs/1602.04484},
	abstract = {We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress.},
	journal = {arXiv:1602.04484 [cs, math, stat]},
	author = {Helmbold, David P. and Long, Philip M.},
	month = feb,
	year = {2016},
	note = {citecount: 00000 
arXiv: 1602.04484},
	keywords = {queue},
	pages = {21},
	file = {Helmbold and Long - 2016 - Surprising properties of dropout in deep networks.pdf:/home/miguel/Zotero/storage/PR85MA3Q/Helmbold and Long - 2016 - Surprising properties of dropout in deep networks.pdf:application/pdf}
}

@article{arias-castro_spectral_2017,
	title = {Spectral {Clustering} based on local {PCA}},
	volume = {18},
	url = {http://jmlr.org/papers/v18/14-318.html},
	number = {9},
	urldate = {2017-04-12},
	journal = {Journal of Machine Learning Research},
	author = {Arias-Castro, Ery and Lerman, Gilad and Zhang, Teng},
	year = {2017},
	note = {citecount: 00025 
code: http://sciences.ucf.edu/math/tengz},
	pages = {1--57},
	file = {Arias-Castro et al. - 2017 - Spectral Clustering Based on Local PCA.pdf:/home/miguel/Zotero/storage/HHKWMCJ6/Arias-Castro et al. - 2017 - Spectral Clustering Based on Local PCA.pdf:application/pdf;comments - arias-castro et al.tm:/home/miguel/Zotero/storage/QKCRZS38/comments - arias-castro et al.tm:text/plain}
}

@article{nevo_identifying_2017,
	title = {Identifying a minimal class of models for high-dimensional data},
	volume = {18},
	url = {http://jmlr.org/papers/v18/16-172.html},
	number = {24},
	urldate = {2017-04-16},
	journal = {Journal of Machine Learning Research},
	author = {Nevo, Daniel and Ritov, Ya'acov},
	year = {2017},
	note = {citecount: 00000},
	pages = {1--29},
	file = {Nevo and Ritov - 2017 - Identifying a Minimal Class of Models for High--di.pdf:/home/miguel/Zotero/storage/23VFUGGP/Nevo and Ritov - 2017 - Identifying a Minimal Class of Models for High--di.pdf:application/pdf;nevo_identifying_2017.tm:/home/miguel/Zotero/storage/JHV225EH/nevo_identifying_2017.tm:text/plain}
}

@article{swirszcz_local_2016,
	title = {Local minima in training of neural networks},
	url = {http://arxiv.org/abs/1611.06310},
	abstract = {There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space.},
	language = {en},
	urldate = {2017-05-03},
	journal = {arXiv:1611.06310 [cs, stat]},
	author = {Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
	month = nov,
	year = {2016},
	note = {citecount: 00001 
arXiv: 1611.06310},
	pages = {12},
	file = {Swirszcz et al. - 2016 - Local minima in training of neural networks.pdf:/home/miguel/Zotero/storage/Z5FZJKR8/Swirszcz et al. - 2016 - Local minima in training of neural networks.pdf:application/pdf}
}

@misc{bartlett_representational_2017,
	address = {Simons Institute},
	type = {Talk},
	title = {Representational and optimization properties of {Deep} {Residual} {Networks}},
	url = {https://simons.berkeley.edu/talks/tba-1},
	language = {en},
	urldate = {2017-05-02},
	author = {Bartlett, Peter},
	month = may,
	year = {2017},
	note = {citecount: 00000 
2681 seconds},
	file = {comments - representation drn.tm:/home/miguel/Zotero/storage/HMTN56IG/comments - representation drn.tm:text/plain}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2017-04-18},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {citecount: 01616 
arXiv: 1502.03167},
	pages = {11},
	file = {Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:/home/miguel/Zotero/storage/V4QV566D/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf}
}

@misc{jordan_gradient-based_2017,
	address = {Simons Institute},
	type = {Talk},
	title = {On {Gradient}-{Based} {Optimization}: {Accelerated}, {Distributed}, {Asynchronous} and {Stochastic}},
	shorttitle = {On {Gradient}-{Based} {Optimization}},
	url = {https://simons.berkeley.edu/talks/michael-jordan-2017-5-2},
	abstract = {Many new theoretical challenges have arisen in the area of gradient-based optimization for large-scale statistical data analysis, driven by the needs of applications and the opportunities provided by new hardware and software platforms. I discuss several recent results in this area, including: (1) a new framework for understanding Nesterov acceleration, obtained by taking a continuous-time, Lagrangian/Hamiltonian perspective, (2) a general theory of asynchronous optimization in multi-processor systems, (3) a computationally-efficient approach to variance reduction, and (4) a primal-dual methodology for gradient-based optimization that targets communication bottlenecks in distributed systems.},
	language = {en},
	urldate = {2017-05-31},
	author = {Jordan, Michael},
	month = may,
	year = {2017},
	note = {citecount: 00000 
youtube: https://youtu.be/VE2ITg\_hGnI
duration: 3725 seconds},
	file = {Jordan - 2017 - On Gradient-Based Optimization Accelerated, Distr.pdf:/home/miguel/Zotero/storage/5MAUVS9E/Jordan - 2017 - On Gradient-Based Optimization Accelerated, Distr.pdf:application/pdf}
}

@article{martius_extrapolation_2016,
	title = {Extrapolation and learning equations},
	url = {http://arxiv.org/abs/1610.02995},
	abstract = {In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified.},
	journal = {arXiv:1610.02995 [cs]},
	author = {Martius, Georg and Lampert, Christoph H.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.02995},
	file = {Martius and Lampert - 2016 - Extrapolation and learning equations.pdf:/home/miguel/Zotero/storage/97Q6R86D/Martius and Lampert - 2016 - Extrapolation and learning equations.pdf:application/pdf}
}