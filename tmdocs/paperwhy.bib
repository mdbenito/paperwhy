
@article{lin_why_2016,
	title = {Why does deep and cheap learning work so well?},
	url = {http://arxiv.org/abs/1608.08225},
	abstract = {We show how the success of deep learning depends not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can be approximated through "cheap learning" with exponentially fewer parameters than generic ones, because they have simplifying properties tracing back to the laws of physics. The exceptional simplicity of physics-based functions hinges on properties such as symmetry, locality, compositionality and polynomial log-probability, and we explore how these properties translate into exceptionally simple neural networks approximating both natural phenomena such as images and abstract representations thereof such as drawings. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to renormalization group procedures. We prove various "no-flattening theorems" showing when such efficient deep networks cannot be accurately approximated by shallow ones without efficiency loss: flattening even linear functions can be costly, and flattening polynomials is exponentially expensive; we use group theoretic techniques to show that n variables cannot be multiplied using fewer than 2{\textasciicircum}n neurons in a single hidden layer.},
	language = {en},
	urldate = {2016-10-26},
	journal = {arXiv:1608.08225 [cond-mat, stat]},
	author = {Lin, Henry W. and Tegmark, Max},
	month = aug,
	year = {2016},
	note = {citecount: 00019 
arXiv: 1608.08225},
	pages = {17},
	file = {Lin and Tegmark - 2016 - Why does deep and cheap learning work so well.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/JD6HE5EB/Lin and Tegmark - 2016 - Why does deep and cheap learning work so well.pdf:application/pdf;tegmark-comments.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/A8V6DEBS/tegmark-comments.tm:text/plain}
}

@misc{bartlett_representational_2017,
	address = {Simons Institute},
	type = {Talk},
	title = {Representational and optimization properties of {Deep} {Residual} {Networks}},
	url = {https://simons.berkeley.edu/talks/tba-1},
	language = {en},
	urldate = {2017-05-02},
	author = {Bartlett, Peter},
	month = may,
	year = {2017},
	note = {citecount: 00000 
2681 seconds},
	file = {Bartlett_2017_Representational and optimization properties of Deep Residual Networks.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/URXCD347/Bartlett_2017_Representational and optimization properties of Deep Residual Networks.tm:text/plain}
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	language = {en},
	urldate = {2017-04-28},
	journal = {arXiv:1207.0580 [cs]},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {citecount: 01870 
arXiv: 1207.0580},
	pages = {18},
	file = {Hinton et al. - 2012 - Improving neural networks by preventing co-adapta.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/MPZEZ35X/Hinton et al. - 2012 - Improving neural networks by preventing co-adapta.pdf:application/pdf;hinton_improving_2012.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/9KCABFBP/hinton_improving_2012.tm:text/plain}
}

@article{baldi_dropout_2014,
	title = {The {Dropout} {Learning} {Algorithm}},
	volume = {210},
	issn = {0004-3702},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3996711/},
	doi = {10.1016/j.artint.2014.02.004},
	abstract = {Dropout is a recently introduced algorithm for training neural network by randomly dropping units during training to prevent their co-adaptation. A mathematical analysis of some of the static and dynamic properties of dropout is provided using Bernoulli gating variables, general enough to accommodate dropout on units or connections, and with variable rates. The framework allows a complete analysis of the ensemble averaging properties of dropout in linear networks, which is useful to understand the non-linear case. The ensemble averaging properties of dropout in non-linear logistic networks result from three fundamental equations: (1) the approximation of the expectations of logistic functions by normalized geometric means, for which bounds and estimates are derived; (2) the algebraic equality between normalized geometric means of logistic functions with the logistic of the means, which mathematically characterizes logistic functions; and (3) the linearity of the means with respect to sums, as well as products of independent variables. The results are also extended to other classes of transfer functions, including rectified linear functions. Approximation errors tend to cancel each other and do not accumulate. Dropout can also be connected to stochastic neurons and used to predict firing rates, and to backpropagation by viewing the backward propagation as ensemble averaging in a dropout linear network. Moreover, the convergence properties of dropout can be understood in terms of stochastic gradient descent. Finally, for the regularization properties of dropout, the expectation of the dropout gradient is the gradient of the corresponding approximation ensemble, regularized by an adaptive weight decay term with a propensity for self-consistent variance minimization and sparse representations.},
	language = {en},
	urldate = {2017-05-02},
	journal = {Artificial intelligence},
	author = {Baldi, Pierre and Sadowski, Peter},
	month = may,
	year = {2014},
	pmid = {24771879},
	pmcid = {PMC3996711},
	note = {citecount: 00068 },
	pages = {78--122},
	file = {Baldi et al_2014_The Dropout Learning Algorithm.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/8GFTJJ4B/Baldi et al_2014_The Dropout Learning Algorithm.pdf:application/pdf}
}

@article{tang_deep_2013,
	title = {Deep learning using linear {Support} {Vector} {Machines}},
	url = {http://arxiv.org/abs/1306.0239},
	abstract = {Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these "deep learning" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.},
	language = {en},
	urldate = {2015-04-29},
	journal = {arXiv:1306.0239 [cs, stat]},
	author = {Tang, Yichuan},
	month = jun,
	year = {2013},
	note = {citecount: 00164 
arXiv: 1306.0239},
	pages = {6},
	file = {comments-deep-svm.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/6MW3T25B/comments-deep-svm.tm:text/plain;Tang - 2013 - Deep Learning using Linear Support Vector Machines copy.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/IIV3JS62/Tang - 2013 - Deep Learning using Linear Support Vector Machines copy.pdf:application/pdf}
}

@article{goodfellow_maxout_2013,
	title = {Maxout {Networks}},
	url = {http://arxiv.org/abs/1302.4389},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
	language = {en},
	urldate = {2017-04-18},
	journal = {arXiv:1302.4389 [cs, stat]},
	author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	month = feb,
	year = {2013},
	note = {citecount: 00756 
code: http://www-etud.iro.umontreal.ca/{\textasciitilde}goodfeli/maxout.html
arxiv: 1302.4389},
	pages = {9},
	file = {Goodfellow et al. - 2013 - Maxout Networks.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/WSJ969AT/Goodfellow et al. - 2013 - Maxout Networks.pdf:application/pdf}
}

@incollection{ng_spectral_2002,
	title = {On {Spectral} {Clustering}: {Analysis} and an algorithm},
	shorttitle = {On {Spectral} {Clustering}},
	url = {http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf},
	urldate = {2017-04-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 14},
	publisher = {MIT Press},
	author = {Ng, Andrew Y. and Jordan, Michael I. and Weiss, Yair},
	editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
	year = {2002},
	note = {citecount: 05818},
	pages = {849--856},
	file = {Ng et al. - 2002 - On Spectral Clustering Analysis and an algorithm.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/V8QWH8QR/Ng et al. - 2002 - On Spectral Clustering Analysis and an algorithm.pdf:application/pdf}
}

@article{nevo_identifying_2017,
	title = {Identifying a minimal class of models for high-dimensional data},
	volume = {18},
	url = {https://paperwhy.aerobatic.io/2017/05/18/deep-sparse-rectifier-neural-networks/},
	number = {24},
	urldate = {2017-04-16},
	journal = {Journal of Machine Learning Research},
	author = {Nevo, Daniel and Ritov, Ya'acov},
	year = {2017},
	note = {citecount: 00000
arXiv:1504.00494},
	pages = {1--29},
	file = {Nevo and Ritov - 2017 - Identifying a Minimal Class of Models for High--di.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/2C4HXW4Q/Nevo and Ritov - 2017 - Identifying a Minimal Class of Models for High--di.pdf:application/pdf;nevo_identifying_2017.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/ICCER7GI/nevo_identifying_2017.tm:text/plain}
}

@article{swirszcz_local_2016,
	title = {Local minima in training of neural networks},
	url = {http://arxiv.org/abs/1611.06310},
	abstract = {There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space.},
	language = {en},
	urldate = {2017-05-03},
	journal = {arXiv:1611.06310 [cs, stat]},
	author = {Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
	month = nov,
	year = {2016},
	note = {citecount: 00001 
arXiv: 1611.06310},
	pages = {12},
	file = {comments-localminima.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/VJCJB7XE/comments-localminima.tm:text/plain;Swirszcz et al. - 2016 - Local minima in training of neural networks.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/I8GERV73/Swirszcz et al. - 2016 - Local minima in training of neural networks.pdf:application/pdf}
}

@article{athiwaratkun_multimodal_2017,
	title = {Multimodal {Word} {Distributions}},
	url = {http://arxiv.org/abs/1704.08424},
	abstract = {Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.},
	urldate = {2017-05-25},
	journal = {arXiv:1704.08424 [cs, stat]},
	author = {Athiwaratkun, Ben and Wilson, Andrew Gordon},
	month = apr,
	year = {2017},
	note = {citecount: 00000 
arXiv: 1704.08424},
	file = {Athiwaratkun et al_2017_Multimodal Word Distributions.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/VKNWTJWF/Athiwaratkun et al_2017_Multimodal Word Distributions.pdf:application/pdf}
}

@incollection{bengio_greedy_2007,
	title = {Greedy layer-wise training of {Deep} {Networks}},
	url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
	urldate = {2017-05-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
	editor = {Schölkopf, P. B. and Platt, J. C. and Hoffman, T.},
	year = {2007},
	note = {citecount: 02175},
	pages = {153--160},
	file = {Bengio et al. - 2007 - Appendix - Greedy Layer-Wise Training of Deep Networks.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/72BRF4IS/Bengio et al. - 2007 - Appendix - Greedy Layer-Wise Training of Deep Networks.pdf:application/pdf;Bengio et al. - 2007 - Greedy Layer-Wise Training of Deep Networks.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/2DV7TVUB/Bengio et al. - 2007 - Greedy Layer-Wise Training of Deep Networks.pdf:application/pdf;comments - greedy layer-wise.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/M6DKKEIN/comments - greedy layer-wise.tm:text/plain}
}

@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1018054314350},
	doi = {10.1023/A:1018054314350},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	language = {en},
	number = {2},
	urldate = {2017-05-23},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = aug,
	year = {1996},
	note = {citecount: 16604},
	pages = {123--140},
	file = {Breiman_1996_Bagging predictors.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/QDKF9BJS/Breiman_1996_Bagging predictors.pdf:application/pdf}
}

@article{vilnis_word_2014,
	title = {Word {Representations} via {Gaussian} {Embedding}},
	url = {http://arxiv.org/abs/1412.6623},
	abstract = {Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.},
	urldate = {2017-05-25},
	journal = {arXiv:1412.6623 [cs]},
	author = {Vilnis, Luke and McCallum, Andrew},
	month = dec,
	year = {2014},
	note = {citecount: 00043 
arXiv: 1412.6623},
	file = {Vilnis et al_2014_Word Representations via Gaussian Embedding.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/69MPBPKU/Vilnis et al_2014_Word Representations via Gaussian Embedding.pdf:application/pdf}
}

@inproceedings{glorot_deep_2011,
	address = {Florida, USA},
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	volume = {15},
	url = {http://www.jmlr.org/proceedings/papers/v15/},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
	language = {en},
	urldate = {2017-04-07},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}  {April} 11-13, 2011, {Fort} {Lauderdale}, {FL}, {USA}},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	month = apr,
	year = {2011},
	note = {citecount: 00935},
	pages = {315--323},
	file = {comments-relu.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/HHNE6QEU/comments-relu.tm:text/plain;Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/R9J6MQD7/Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf:application/pdf}
}

@incollection{baldi_understanding_2013,
	title = {Understanding {Dropout}},
	url = {http://papers.nips.cc/paper/4878-understanding-dropout.pdf},
	abstract = {Dropout is a relatively new algorithm for training neural networks which relies on stochastically dropping out neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. We also show in simple cases how dropout performs stochastic gradient descent on a regularized error function.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	author = {Baldi, Pierre and Sadowski, Peter J},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	note = {citecount: 00066},
	pages = {2814--2822},
	file = {Baldi and Sadowski - 2013 - Understanding Dropout.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/7H3AJGB9/Baldi and Sadowski - 2013 - Understanding Dropout.pdf:application/pdf;comments-understanding-dropout.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/935UE3TS/comments-understanding-dropout.tm:text/plain}
}

@inproceedings{nair_rectified_2010,
	address = {Haifa, Israel},
	title = {Rectified linear units improve restricted boltzmann machines},
	language = {en},
	booktitle = {Proceedings of the 27th international conference on machine learning ({ICML}-10)},
	author = {Nair, Vinod and Hinton, Geoffrey E},
	year = {2010},
	note = {citecount: 01616},
	pages = {807--814},
	file = {Nair et al_2010_Rectified linear units improve restricted boltzmann machines.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/7A7E8KBZ/Nair et al_2010_Rectified linear units improve restricted boltzmann machines.pdf:application/pdf}
}

@article{arias-castro_spectral_2017,
	title = {Spectral {Clustering} based on local {PCA}},
	volume = {18},
	url = {http://jmlr.org/papers/v18/14-318.html},
	number = {9},
	urldate = {2017-04-12},
	journal = {Journal of Machine Learning Research},
	author = {Arias-Castro, Ery and Lerman, Gilad and Zhang, Teng},
	year = {2017},
	note = {citecount: 00025 
code: http://sciences.ucf.edu/math/tengz},
	pages = {1--57},
	file = {Arias-Castro et al. - 2017 - Spectral Clustering Based on Local PCA.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/54HPC7MI/Arias-Castro et al. - 2017 - Spectral Clustering Based on Local PCA.pdf:application/pdf;comments - arias-castro et al.tm:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/4M3AWNQ7/comments - arias-castro et al.tm:text/plain}
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	language = {en},
	urldate = {2017-05-02},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	note = {citecount: 02156 
Extended version with appendix from the Arxiv},
	pages = {770--778},
	file = {He et al. - 2015 - Deep Residual Learning - with appendix.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/URSIRXXI/He et al. - 2015 - Deep Residual Learning - with appendix.pdf:application/pdf;He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/XIS8MDIJ/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	language = {en},
	urldate = {2017-05-02},
	journal = {arXiv:1603.05027 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = mar,
	year = {2016},
	note = {citecount: 00247 
arXiv: 1603.05027},
	pages = {15},
	file = {He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/J9ZDITST/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:application/pdf}
}

@unpublished{schmidt_minimizing_2016,
	title = {Minimizing {Finite} {Sums} with the {Stochastic} {Average} {Gradient}},
	url = {https://hal.inria.fr/hal-00860051},
	language = {en},
	author = {Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
	month = may,
	year = {2016},
	note = {citecount: 00250},
	file = {Schmidt et al. - 2016 - Minimizing Finite Sums with the Stochastic Average.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/PA9X6F7U/Schmidt et al. - 2016 - Minimizing Finite Sums with the Stochastic Average.pdf:application/pdf}
}

@article{vidal_subspace_2011,
	title = {Subspace {Clustering}},
	volume = {28},
	issn = {1053-5888},
	doi = {10.1109/MSP.2010.939739},
	abstract = {Over the past few decades, significant progress has been made in clustering high-dimensional data sets distributed around a collection of linear and affine subspaces. This article presented a review of such progress, which included a number of existing subspace clustering algorithms together with an experimental evaluation on the motion segmentation and face clustering problems in computer vision.},
	number = {2},
	journal = {IEEE Signal Processing Magazine},
	author = {Vidal, R.},
	month = mar,
	year = {2011},
	note = {citecount: 00774},
	pages = {52--68},
	file = {Vidal - 2011 - Subspace Clustering.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/XWTS5Z7Z/Vidal - 2011 - Subspace Clustering.pdf:application/pdf}
}

@article{wang_general_2004,
	title = {General constructive representations for continuous piecewise-linear functions},
	volume = {51},
	issn = {1549-8328},
	doi = {10.1109/TCSI.2004.834521},
	abstract = {The problem of constructing a canonical representation for an arbitrary continuous piecewise-linear (PWL) function in any dimension is considered in this paper. We solve the problem based on a general lattice PWL representation, which can be determined for a given continuous PWL function using existing methods. We first transform the lattice PWL representation into the difference of two convex functions, then propose a constructive procedure to rewrite the latter as a canonical representation that consists of at most n-level nestings of absolute-value functions in n dimensions, hence give a thorough solution to the problem mentioned above. In addition, we point out that there exist notable differences between a lattice representation and the two novel general constructive representations proposed in this paper, and explain that these differences make all the three representations be of their particular interests.},
	number = {9},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Wang, Shuning},
	month = sep,
	year = {2004},
	note = {citecount: 00014},
	pages = {1889--1896}
}

@article{pinkus_approximation_1999,
	title = {Approximation theory of the {MLP} model in neural networks},
	volume = {8},
	url = {http://journals.cambridge.org/abstract_s0962492900002919},
	urldate = {2017-05-29},
	journal = {Acta Numerica},
	author = {Pinkus, Allan},
	year = {1999},
	note = {citecount: 00333},
	pages = {143--195},
	file = {Pinkus - 1999 - Approximation theory of the MLP model in neural ne.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/KC52QJD2/Pinkus - 1999 - Approximation theory of the MLP model in neural ne.pdf:application/pdf}
}

@article{poggio_why_2016,
	title = {Why and {When} {Can} {Deep} -- but {Not} {Shallow} -- {Networks} {Avoid} the {Curse} of {Dimensionality}: a {Review}},
	shorttitle = {Why and {When} {Can} {Deep} -- but {Not} {Shallow} -- {Networks} {Avoid} the {Curse} of {Dimensionality}},
	url = {http://arxiv.org/abs/1611.00740},
	abstract = {The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning. Deep convolutional networks are a special case of these conditions, though weight sharing is not the main reason for their exponential advantage.},
	urldate = {2017-04-07},
	journal = {arXiv:1611.00740 [cs]},
	author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
	month = nov,
	year = {2016},
	note = {citecount: 00002 
arXiv: 1611.00740},
	pages = {26},
	file = {Poggio et al. - 2016 - Why and When Can Deep -- but Not Shallow -- Networ.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/THBSRBNS/Poggio et al. - 2016 - Why and When Can Deep -- but Not Shallow -- Networ.pdf:application/pdf}
}

@incollection{wager_dropout_2013,
	title = {Dropout training as adaptive regularization},
	url = {http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	author = {Wager, Stefan and Wang, Sida and Liang, Percy},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	note = {citecount: 00156},
	pages = {351--359},
	file = {Appendix.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/DIVNE53N/Appendix.pdf:application/pdf;Wager et al. - 2013 - Dropout Training as Adaptive Regularization.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/WC5CCE9A/Wager et al. - 2013 - Dropout Training as Adaptive Regularization.pdf:application/pdf}
}

@article{duchi_adaptive_2011,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v12/duchi11a.html},
	number = {Jul},
	urldate = {2016-07-31},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	note = {citecount: 01743},
	pages = {2121--2159},
	file = {Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/BZ748GK7/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf}
}

@article{bishop_training_1995,
	title = {Training with noise is equivalent to {Tikhonov} regularization},
	volume = {7},
	number = {1},
	journal = {Neural computation},
	author = {Bishop, Chris M},
	year = {1995},
	note = {citecount: 00545},
	pages = {108--116},
	file = {Bishop - 1995 - Training with noise is equivalent to Tikhonov regu.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/GQ6JMP3U/Bishop - 1995 - Training with noise is equivalent to Tikhonov regu.pdf:application/pdf}
}

@article{hardt_identity_2016,
	title = {Identity matters in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1611.04231},
	abstract = {An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as {\textbackslash}emph\{batch normalization\}, but was also key to the immense success of {\textbackslash}emph\{residual networks\}. In this work, we put the principle of {\textbackslash}emph\{identity parameterization\} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.},
	journal = {arXiv:1611.04231 [cs, stat]},
	author = {Hardt, Moritz and Ma, Tengyu},
	month = nov,
	year = {2016},
	note = {citecount: 00001 
arXiv: 1611.04231},
	file = {Hardt and Ma - 2016 - Identity matters in Deep Learning.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/HIP545D2/Hardt and Ma - 2016 - Identity matters in Deep Learning.pdf:application/pdf}
}

@inproceedings{sutskever_importance_2013,
	title = {On the importance of initialization and momentum in deep learning},
	url = {http://proceedings.mlr.press/v28/sutskever13.html},
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this pa...},
	language = {en},
	urldate = {2017-06-01},
	booktitle = {{PMLR}},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey E.},
	month = feb,
	year = {2013},
	note = {citecount: 00593},
	pages = {1139--1147},
	file = {Sutskever et al. - 2013 - On the importance of initialization and momentum i.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/3KD9GB4E/Sutskever et al. - 2013 - On the importance of initialization and momentum i.pdf:application/pdf;Sutskever et al. - 2013 - On the importance - Supplementary material.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/CBSZ96I7/Sutskever et al. - 2013 - On the importance - Supplementary material.pdf:application/pdf}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2017-04-18},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {citecount: 01618 
arXiv: 1502.03167},
	pages = {11},
	file = {Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/CDCNFX6S/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf}
}

@article{su_differential_2016,
	title = {A differential equation for modeling {Nesterov}'s accelerated gradient method: {Theory} and insights},
	volume = {17},
	shorttitle = {A {Differential} {Equation} for {Modeling} {Nesterov}'s {Accelerated} {Gradient} {Method}},
	url = {http://jmlr.org/papers/v17/15-084.html},
	number = {153},
	urldate = {2017-05-31},
	journal = {Journal of Machine Learning Research},
	author = {Su, Weijie and Boyd, Stephen and Candès, Emmanuel J.},
	year = {2016},
	note = {citecount: 00083 
arXiv: 1503.01243},
	pages = {1--43},
	file = {Su et al. - 2016 - A Differential Equation for Modeling Nesterov's Ac.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/J6Z8K5CW/Su et al. - 2016 - A Differential Equation for Modeling Nesterov's Ac.pdf:application/pdf}
}

@book{nesterov_introductory_2004,
	series = {Applied {Optimization}},
	title = {Introductory {Lectures} on {Convex} {Optimization} - {A} {Basic} {Course}},
	isbn = {978-1-4613-4691-3},
	url = {http://www.springer.com/de/book/9781402075537},
	abstract = {It was in the middle of the 1980s, when the seminal paper by Kar­ markar opened a new epoch in nonlinear optimization. The importance of this paper,...},
	language = {en},
	number = {87},
	urldate = {2017-06-03},
	publisher = {Springer},
	author = {Nesterov, Yurii},
	year = {2004},
	note = {citecount: 02412},
	file = {Nesterov - 2004 - Introductory Lectures on Convex Optimization - A B.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/8S2XQA3I/Nesterov - 2004 - Introductory Lectures on Convex Optimization - A B.pdf:application/pdf}
}

@misc{jordan_gradient-based_2017,
	address = {Simons Institute},
	type = {Talk},
	title = {On {Gradient}-{Based} {Optimization}: {Accelerated}, {Distributed}, {Asynchronous} and {Stochastic}},
	shorttitle = {On {Gradient}-{Based} {Optimization}},
	url = {https://simons.berkeley.edu/talks/michael-jordan-2017-5-2},
	abstract = {Many new theoretical challenges have arisen in the area of gradient-based optimization for large-scale statistical data analysis, driven by the needs of applications and the opportunities provided by new hardware and software platforms. I discuss several recent results in this area, including: (1) a new framework for understanding Nesterov acceleration, obtained by taking a continuous-time, Lagrangian/Hamiltonian perspective, (2) a general theory of asynchronous optimization in multi-processor systems, (3) a computationally-efficient approach to variance reduction, and (4) a primal-dual methodology for gradient-based optimization that targets communication bottlenecks in distributed systems.},
	language = {en},
	urldate = {2017-05-31},
	author = {Jordan, Michael},
	month = may,
	year = {2017},
	note = {citecount: 00000 
youtube: https://youtu.be/VE2ITg\_hGnI
duration: 3725 seconds},
	file = {Jordan - 2017 - On Gradient-Based Optimization Accelerated, Distr.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/JZHCFHTK/Jordan - 2017 - On Gradient-Based Optimization Accelerated, Distr.pdf:application/pdf}
}

@article{nesterov_method_1983,
	title = {A method of solving a convex programming problem with convergence rate {O} (1/k2)},
	volume = {27},
	url = {http://www.core.ucl.ac.be/~nesterov/Research/Papers/DAN83.pdf},
	journal = {Soviet Mathematics Doklady},
	author = {Nesterov, Yurii},
	year = {1983},
	note = {citecount: 01269},
	pages = {372--376}
}

@article{saxe_exact_2013,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
	url = {http://arxiv.org/abs/1312.6120},
	abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
	journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
	author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
	month = dec,
	year = {2013},
	note = {citecount: 00174 
arXiv: 1312.6120},
	file = {Saxe et al. - 2013 - Exact solutions to the nonlinear dynamics of learn.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/BDXN9JWC/Saxe et al. - 2013 - Exact solutions to the nonlinear dynamics of learn.pdf:application/pdf}
}